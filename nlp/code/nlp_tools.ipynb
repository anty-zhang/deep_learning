{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLPç®—æ³•å·¥å…·é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# tfidf\n",
    "# LDA\n",
    "# textrank\n",
    "# word2vec\n",
    "# doc2vec\n",
    "# bert\n",
    "# s-bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬é¢„å¤„ç†å±‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å»é™¤ç‰¹æ®Šç¬¦å·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "#æ ¹æ®æ–‡æœ¬çš„ç‰¹å¾ï¼Œç¡®å®šéœ€è¦å¤„ç†çš„ä¸€äº›ç‰¹æ®Šç¬¦å·\n",
    "def preprocess4wb(text_content):\n",
    "\n",
    "    # è¿‡æ»¤æ–‡æœ¬ä¸­çš„htmlé“¾æ¥ç­‰\n",
    "    re_tag = re.compile('</?\\w+[^>]*>')  # HTMLæ ‡ç­¾\n",
    "    new_text = re.sub(re_tag, '', text_content)\n",
    "    new_text = re.sub(u\"\\\\[.*?]\", \"\", new_text) #å»é™¤[]åŠå…¶å†…éƒ¨å†…å®¹ï¼ˆè¡¨æƒ…ç­‰ä¼šæ˜¯è¿™æ ·ï¼‰\n",
    "    new_text = emoji.replace_emoji(new_text,'')  # å»é™¤ emoji\n",
    "    new_text = re.sub(\",+\", \",\", new_text)  # åˆå¹¶é€—å·\n",
    "    new_text = re.sub(\" +\", \" \", new_text)  # åˆå¹¶ç©ºæ ¼\n",
    "    new_text = re.sub(\"[...|â€¦|ã€‚ã€‚ã€‚]+\", \"...\", new_text)  # åˆå¹¶å¥å·\n",
    "    new_text = re.sub(\"-+\", \"--\", new_text)  # åˆå¹¶-\n",
    "    text_content = re.sub(\"â€”â€”â€”+\", \"â€”â€”â€”\", new_text)  # åˆå¹¶-\n",
    "    topic = re.findall('#[^#]+#', str(text_content))#å»é™¤è¯é¢˜æ ‡è¯†\n",
    "    name = re.findall('@[\\\\u4e00-\\\\u9fa5\\\\w\\\\-]+', str(text_content)) #å»é™¤è‰¾ç‰¹\n",
    "    if topic!=None:\n",
    "        for tp in topic:\n",
    "            text_content=text_content.replace(tp,\"\")\n",
    "    if name!=None:\n",
    "        for nm in name:\n",
    "            text_content=text_content.replace(nm,\"\")\n",
    "    text_content=text_content.replace(\" \",\"\")\n",
    "    punctuation = '~`!#$%^&*()_+-=|\\';\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{}ã€Œã€ã€ã€‘'\n",
    "    text_content = re.sub(r\"[%s]+\" %punctuation, \"\",text_content)\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ï¼Ÿã€Œã€[asda]123è¿™æ˜¯ä¸€ä¸ªæµ‹...--..ã€‚ã€‚ã€‚ã€‚è¯•æ–‡æœ¬<fffff> #è¯é¢˜# @23æµ·è´¼ç‹ğŸ™ƒğŸ˜ğŸ˜…ğŸ˜˜ğŸ˜’ '\n",
    "rs=preprocess4wb(text)\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†è¯ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/cv/39t5r19s6s94k9_p328_hnv00000gp/T/jieba.cache\n",
      "Loading model cost 0.324 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æœ¬æ–‡', 'è¯¦ç»†ä»‹ç»', 'ç”Ÿæˆ', 'å¯¹æŠ—', 'ç½‘ç»œ', ' ', 'â€“', ' ', 'GAN', ' ', 'è®¾è®¡', 'åˆè¡·', 'åŸºæœ¬åŸç†', '10', 'å…¸å‹', 'ç®—æ³•', '13', 'å®é™…åº”ç”¨']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "load_dict_flag = False\n",
    "def load_dict():\n",
    "    # åŠ è½½è¯å…¸\n",
    "    if not load_dict_flag:\n",
    "        jieba.load_userdict(\"../data/dict/SogouLabDic.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_car.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_baidu_utf8.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_pangu.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_sougou_utf8.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_tencent_utf8.txt\")\n",
    "\n",
    "\n",
    "def cut_wd(content):\n",
    "    load_dict()\n",
    "    stopwords = {}.fromkeys([line.rstrip() for line in open('../data/dict/Stopword.txt',encoding='utf-8')])\n",
    "    seg = jieba.cut(content)\n",
    "    wordslist=[]\n",
    "    for i in seg:\n",
    "        if i not in stopwords:\n",
    "            wordslist.append(i)\n",
    "#     wordstext=\" \".join(wordslist)\n",
    "    return wordslist\n",
    "text='æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ â€“ GAN çš„è®¾è®¡åˆè¡·ã€åŸºæœ¬åŸç†ã€10ç§å…¸å‹ç®—æ³•å’Œ13ç§å®é™…åº”ç”¨'\n",
    "wdlist=cut_wd(text)\n",
    "print(wdlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†å¥ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ â€“ GAN çš„è®¾è®¡åˆè¡·ã€', 'åŸºæœ¬åŸç†ã€', '10ç§å…¸å‹ç®—æ³•å’Œ13ç§å®é™…åº”ç”¨']\n"
     ]
    }
   ],
   "source": [
    "def cut_sentences(content):\n",
    "    # ç»“æŸç¬¦å·ï¼ŒåŒ…å«ä¸­æ–‡å’Œè‹±æ–‡çš„,ä¾æ®å…·ä½“çš„éœ€æ±‚è®¾å®šåˆ’åˆ†ä¾æ®\n",
    "    end_flag = ['?', '!', '.', 'ï¼Ÿ', 'ï¼', 'ã€‚',';', 'â€¦', 'ã€']\n",
    "\n",
    "    content_len = len(content)\n",
    "    sentences = []\n",
    "    tmp_char = ''\n",
    "    for idx, char in enumerate(content):\n",
    "        # æ‹¼æ¥å­—ç¬¦\n",
    "        tmp_char += char\n",
    "\n",
    "        # åˆ¤æ–­æ˜¯å¦å·²ç»åˆ°äº†æœ€åä¸€ä½\n",
    "        if (idx + 1) == content_len:\n",
    "            sentences.append(tmp_char)\n",
    "            break\n",
    "\n",
    "        # åˆ¤æ–­æ­¤å­—ç¬¦æ˜¯å¦ä¸ºç»“æŸç¬¦å·\n",
    "        if char in end_flag:\n",
    "            # å†åˆ¤æ–­ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦ä¸ºç»“æŸç¬¦å·ï¼Œå¦‚æœä¸æ˜¯ç»“æŸç¬¦å·ï¼Œåˆ™åˆ‡åˆ†å¥å­\n",
    "            next_idx = idx + 1\n",
    "            if not content[next_idx] in end_flag:\n",
    "                sentences.append(tmp_char)\n",
    "                tmp_char = ''\n",
    "\n",
    "    return sentences\n",
    "\n",
    "text='æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ â€“ GAN çš„è®¾è®¡åˆè¡·ã€åŸºæœ¬åŸç†ã€10ç§å…¸å‹ç®—æ³•å’Œ13ç§å®é™…åº”ç”¨'\n",
    "rs=cut_sentences(text)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®ä½“æå–-è¯æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T01:59:07.073273Z",
     "start_time": "2021-12-17T01:59:06.990590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns -> ç¾å›½\n",
      "nr -> é©¬äº‘\n",
      "nr -> å­™æ­£ä¹‰\n",
      "ns -> å—é\n",
      "ns -> å—é\n",
      "ns -> éæ´²\n",
      "nt -> ä¸Šå¸‚å…¬å¸\n",
      "nr -> é©¬åŒ–è…¾\n",
      "nr -> é©¬åŒ–è…¾\n"
     ]
    }
   ],
   "source": [
    "# å¼•å…¥è¯æ€§æ ‡æ³¨æ¥å£\n",
    "import jieba.posseg as psg\n",
    " \n",
    "text = '''ç½‘å‹ä»¬éƒ½æ¸…æ¥šï¼Œä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸šï¼Œä¸€æ–¹é¢æ˜¯å¾—ç›Šäºä¸­å›½äº’è”ç½‘åœ¨å•†ä¸šæ¨¡å¼ä¸Šçš„åˆ›æ–°ï¼Œå¦ä¸€æ–¹é¢åˆ™æ˜¯å—ç›Šäºèµ„æœ¬çš„é©±åŠ¨ã€‚\n",
    "é˜¿é‡Œå·´å·´è¿™å®¶å…¬å¸æ—©å¹´ä¹Ÿå·®é’±ï¼Œé©¬äº‘çš„é˜¿é‡Œè·å¾—äº†å­™æ­£ä¹‰è½¯é“¶çš„2000ä¸‡ç¾å…ƒæŠ•èµ„ã€‚è‡ªæ­¤é˜¿é‡Œå·´å·´çš„B2Bä¸šåŠ¡å¼€å§‹è¿›å†›å›½é™…å¸‚åœºï¼Œå¹¶åœ¨å›½é™…å¸‚åœºä¸Šæ‰“å‡ºäº†åæ°”ã€‚\n",
    "åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚è…¾è®¯çš„å¤§è‚¡ä¸œæ˜¯å—éæŠ¥ä¸šé›†å›¢ã€‚\n",
    "å¤§è‚¡ä¸œNaspersæ˜¯ä¸€å®¶1915å¹´æˆç«‹ï¼Œæ€»éƒ¨ä½äºå—éçš„ä¼ åª’é›†å›¢ã€‚2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼ã€‚\n",
    "ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚'''\n",
    "#è¯æ€§æ ‡æ³¨\n",
    "seg = psg.cut(text)\n",
    " \n",
    "#å°†è¯æ€§æ ‡æ³¨ç»“æœæ‰“å°å‡ºæ¥\n",
    "for word,flag in seg:\n",
    "    if flag in ('ns', 'nr', 'nt'):\n",
    "        print (flag, '->', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¼–ç "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ç½‘å‹,ä¸­å›½äº’è”ç½‘,è¡Œä¸š,åª²ç¾,ç¾å›½,äº’è”ç½‘,è¡Œä¸š,å¾—ç›Šäº,ä¸­å›½äº’è”ç½‘,å•†ä¸šæ¨¡å¼,åˆ›æ–°,åˆ™æ˜¯,å—ç›Š,èµ„æœ¬,é©±åŠ¨', 'é˜¿é‡Œå·´å·´,è¿™å®¶,å…¬å¸,æ—©å¹´,å·®é’±,é©¬äº‘,é˜¿é‡Œ,è·å¾—äº†,å­™æ­£ä¹‰,è½¯é“¶,ä¸‡ç¾å…ƒ,æŠ•èµ„,è‡ªæ­¤,é˜¿é‡Œå·´å·´,BB,ä¸šåŠ¡,è¿›å†›,å›½é™…å¸‚åœº,å¹¶åœ¨,å›½é™…å¸‚åœº,æ‰“å‡ºäº†,åæ°”', 'è…¾è®¯,å…¬å¸,æ—©æœŸ,è·å¾—äº†,èµ„æœ¬,æ”¯æŒ,å–å¾—äº†,å¿«é€Ÿå‘å±•,è…¾è®¯,å¤§è‚¡ä¸œ,å—é,æŠ¥ä¸šé›†å›¢', 'å¤§è‚¡ä¸œ,Naspers,æ˜¯ä¸€å®¶,å¹´,æˆç«‹,æ€»éƒ¨,ä½äº,å—é,ä¼ åª’,é›†å›¢,å¹´,è´­ä¹°äº†,è…¾è®¯,è‚¡ä»½,è…¾è®¯,å¸‚å€¼,é£™æ¶¨,Naspers,éæ´²,å¸‚å€¼,ä¸Šå¸‚å…¬å¸,è…¾è®¯,è‚¡æƒ,å¸‚å€¼,è¶…è¿‡äº†,ä¸šåŠ¡,å¸‚å€¼', 'æ•°æ®,å¯ä»¥çœ‹å‡º,é©¬åŒ–è…¾,é©¬åŒ–è…¾,åŸºé‡‘,è‚¡ä»½,æŒè‚¡,ä¸åˆ°,è…¾è®¯,æ—©æœŸ,äº”è™,å¥—ç°,å‡ºç°åœ¨,å¤§è‚¡ä¸œ,æ¦œå•,ä¹‹ä¸­']\n",
      "TfidfVector_Sklean Time:  0.0015467079999993416\n"
     ]
    }
   ],
   "source": [
    "# TF-IDFï¼ˆTerm Frequencyâ€“Inverse Document Frequencyï¼‰æ˜¯ä¸€ç§ç”¨äºèµ„è®¯æ£€ç´¢ä¸æ–‡æœ¬æŒ–æ˜çš„å¸¸ç”¨åŠ æƒæŠ€æœ¯ã€‚\n",
    "# TF-IDFæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€ä¸ªå­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚\n",
    "# å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™ã€‚\n",
    "# TF-IDFåŠ æƒçš„å„ç§å½¢å¼å¸¸è¢«æœç´¢å¼•æ“åº”ç”¨ï¼Œä½œä¸ºæ–‡ä»¶ä¸ç”¨æˆ·æŸ¥è¯¢ä¹‹é—´ç›¸å…³ç¨‹åº¦çš„åº¦é‡æˆ–è¯„çº§ã€‚\n",
    "\n",
    "# TF-IDFçš„ä¸»è¦æ€æƒ³æ˜¯ï¼šå¦‚æœæŸä¸ªè¯æˆ–çŸ­è¯­åœ¨ä¸€ç¯‡æ–‡ç« ä¸­å‡ºç°çš„é¢‘ç‡TFé«˜ï¼Œå¹¶ä¸”åœ¨å…¶ä»–æ–‡ç« ä¸­å¾ˆå°‘å‡ºç°ï¼Œåˆ™è®¤ä¸ºæ­¤è¯æˆ–è€…çŸ­è¯­å…·æœ‰å¾ˆå¥½çš„ç±»åˆ«åŒºåˆ†èƒ½åŠ›ï¼Œé€‚åˆç”¨æ¥åˆ†ç±»ã€‚\n",
    "# TF-IDFå®é™…ä¸Šæ˜¯ï¼šTF * IDFã€‚\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import timeit\n",
    "vectorizer = TfidfVectorizer()   # å®šä¹‰ä¸€ä¸ªtf-idfçš„vectorizer\n",
    "\n",
    "# æ„å»ºTFIDFçŸ©é˜µï¼Œæ–¹ä¾¿åç»­å¯¹æ¯”ä½¿ç”¨\n",
    "def TfidfVector(wdlist):\n",
    "    start = timeit.default_timer()\n",
    "    X_tfidf = vectorizer.fit_transform(wdlist)   # ç»“æœå­˜æ”¾åœ¨XçŸ©é˜µ\n",
    "    stop = timeit.default_timer()\n",
    "    print('TfidfVector_Sklean Time: ', stop - start)\n",
    "    return X_tfidf\n",
    "\n",
    "def seg_depart(sentence):\n",
    "    # å¯¹æ–‡æ¡£ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œä¸­æ–‡åˆ†è¯\n",
    "\n",
    "    sentence=preprocess4wb(sentence)\n",
    "    sentence_depart = cut_wd(sentence.strip())\n",
    "    return sentence_depart\n",
    "\n",
    "#æµ‹è¯•ä½¿ç”¨ï¼Œæ–‡æœ¬é¦–å…ˆè¿›è¡Œä¸Šé¢ç¬¬ä¸€æ­¥çš„é¢„å¤„ç†\n",
    "quelist=['ç½‘å‹ä»¬éƒ½æ¸…æ¥šï¼Œä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸šï¼Œä¸€æ–¹é¢æ˜¯å¾—ç›Šäºä¸­å›½äº’è”ç½‘åœ¨å•†ä¸šæ¨¡å¼ä¸Šçš„åˆ›æ–°ï¼Œå¦ä¸€æ–¹é¢åˆ™æ˜¯å—ç›Šäºèµ„æœ¬çš„é©±åŠ¨ã€‚',\n",
    "'é˜¿é‡Œå·´å·´è¿™å®¶å…¬å¸æ—©å¹´ä¹Ÿå·®é’±ï¼Œé©¬äº‘çš„é˜¿é‡Œè·å¾—äº†å­™æ­£ä¹‰è½¯é“¶çš„2000ä¸‡ç¾å…ƒæŠ•èµ„ã€‚è‡ªæ­¤é˜¿é‡Œå·´å·´çš„B2Bä¸šåŠ¡å¼€å§‹è¿›å†›å›½é™…å¸‚åœºï¼Œå¹¶åœ¨å›½é™…å¸‚åœºä¸Šæ‰“å‡ºäº†åæ°”ã€‚',\n",
    "'åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚è…¾è®¯çš„å¤§è‚¡ä¸œæ˜¯å—éæŠ¥ä¸šé›†å›¢ã€‚',\n",
    "'å¤§è‚¡ä¸œNaspersæ˜¯ä¸€å®¶1915å¹´æˆç«‹ï¼Œæ€»éƒ¨ä½äºå—éçš„ä¼ åª’é›†å›¢ã€‚2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼ã€‚',\n",
    "'ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚',]\n",
    "wdlist=[]\n",
    "for ques in quelist:\n",
    "    wd=seg_depart(ques)\n",
    "    wdstr=\",\".join(wd)\n",
    "    wdlist.append(wdstr)\n",
    "print(wdlist)\n",
    "\n",
    "X_tfidf = TfidfVector(wdlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 67)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.46310547, 0.        , 0.23155274, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.23155274,\n",
       "        0.23155274, 0.        , 0.        , 0.23155274, 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.23155274, 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46310547, 0.        , 0.18681529,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274],\n",
       "       [0.20018928, 0.        , 0.20018928, 0.        , 0.        ,\n",
       "        0.16151145, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.16151145, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.        , 0.40037855, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.20018928, 0.20018928, 0.        ,\n",
       "        0.20018928, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.20018928, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20018928, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.16151145, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20018928, 0.20018928, 0.20018928, 0.20018928,\n",
       "        0.40037855, 0.        , 0.        , 0.        , 0.20018928,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.26179773, 0.        , 0.        ,\n",
       "        0.        , 0.26179773, 0.32449154, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21731577,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32449154, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32449154, 0.        , 0.32449154,\n",
       "        0.        , 0.        , 0.26179773, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.43463153,\n",
       "        0.        , 0.26179773, 0.        , 0.        , 0.26179773,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.32258952, 0.        , 0.16129476, 0.        ,\n",
       "        0.1301316 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16129476, 0.16129476, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.1301316 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.10802098,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.64517904,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.16129476,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.        ,\n",
       "        0.        , 0.        , 0.1301316 , 0.16129476, 0.32406294,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.        ,\n",
       "        0.16129476, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.16129476, 0.16129476, 0.16129476, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.24846074, 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.16639706,\n",
       "        0.24846074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.        ,\n",
       "        0.24846074, 0.        , 0.20045656, 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.20045656, 0.        , 0.16639706,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.49692149, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00545364 -0.00149582  0.03971366  0.06125599 -0.05921654 -0.04765382\n",
      "  0.05539985  0.05652928 -0.03096416 -0.02589266  0.04575386 -0.00653471\n",
      " -0.02678722  0.03472119 -0.04110786 -0.01157157]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "# inpä¸ºè¾“å…¥è¯­æ–™, outp1ä¸ºè¾“å‡ºæ¨¡å‹, outp2ä¸ºvectoræ ¼å¼çš„æ¨¡å‹\n",
    "input_file = '../data/corpus.txt'\n",
    "out_model = '../data/corpus.model'\n",
    "out_vector = '../data/corpus.vector'\n",
    "\n",
    "# è®­ç»ƒskip-gramæ¨¡å‹ï¼Œ16ç»´ï¼Œæ»‘åŠ¨çª—å£3ï¼Œå‡ºç°è¿‡1è¯å°±ä¿å­˜è¯¥è¯\n",
    "model = Word2Vec(LineSentence(input_file), vector_size=16, window=3, min_count=1, workers=multiprocessing.cpu_count())\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "model.save(out_model)\n",
    "# ä¿å­˜è¯å‘é‡\n",
    "model.wv.save_word2vec_format(out_vector, binary=False)\n",
    "\n",
    "\n",
    "#è°ƒç”¨è®­ç»ƒå¥½çš„word2vecæ¨¡å‹è¿›è¡Œå‘é‡åŒ–\n",
    "\n",
    "import gensim\n",
    "word2vec = Word2Vec.load(\"../data/corpus.model\")\n",
    "\n",
    "# query=\"ä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸š\"\n",
    "# querywd=seg_depart(query)\n",
    "print(word2vec.wv['æ—è‚¯'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertå¥å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:00<00:00, 351kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.99k/1.99k [00:00<00:00, 399kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.00/2.00 [00:00<00:00, 405B/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [00:00<00:00, 292kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.7M/13.7M [00:09<00:00, 1.47MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 21.8kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269k/269k [00:00<00:00, 281kB/s]  \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.0/19.0 [00:00<00:00, 6.98kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110k/110k [00:00<00:00, 152kB/s]  \n",
      "No sentence-transformers model found with name /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of emb is ===== 2\n",
      "[ 0.15300061 -0.09790983  0.86315787  0.04022079  0.34487978  0.04289062\n",
      " -0.15589474 -0.39481714  0.47034568 -0.38537207 -0.07860257  0.09301241\n",
      "  0.20630446 -0.34668624 -0.09810054  0.41245574 -0.15701818 -0.3200327\n",
      " -1.0217427  -0.05748152  0.52095217 -0.27196616  0.14191824 -0.18743883\n",
      "  0.08644073 -0.08078568 -0.01523015 -0.38654917 -0.37808013  0.0541053\n",
      "  0.40847737  0.05551349  0.06039735 -0.0669658   0.25195187 -0.16943024\n",
      "  0.17551786  0.00963114 -0.31012657  0.19530302 -0.1198843   0.09573595\n",
      " -0.05866819 -0.2962663   0.3415701  -0.3293378  -0.09927104  0.36055034\n",
      "  0.11020167  0.24922141 -0.07156698  0.25878298  0.2994059   0.07062481\n",
      " -0.2730424   0.12223209 -0.21613425 -0.26697043 -0.03332004 -0.10973013\n",
      "  0.61800194  0.26704264 -0.14203128 -0.3525691 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentences are mapped to sentence embeddings\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# è¿™é‡Œå¯ä»¥é€‰ç”¨ä¸åŒçš„å¼€æºå‡ºæ¥çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ— éå°±æ˜¯æ¨¡å‹å¤§å°ä¸åŒï¼Œæœ€åçš„å‘é‡è¡¨å¾ä¸åŒ\n",
    "# embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "embedder = SentenceTransformer('hfl/chinese-electra-180g-small-generator')\n",
    "corpus=['å¥”é©°','å®é©¬']\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "print(\"len of emb is =====\", len(corpus_embeddings))\n",
    "print(corpus_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¦»çº¿è®­ç»ƒå±‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFAç®—æ³•ï¼ˆæ–‡æœ¬åŒ¹é…å…³é”®è¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# DFAç®—æ³•\n",
    "class DFAFilter():\n",
    "    def __init__(self):\n",
    "        self.keyword_chains = {}\n",
    "        self.delimit = '\\x00'\n",
    "\n",
    "    def add(self, keyword):\n",
    "        keyword = keyword.lower()\n",
    "        chars = keyword.strip()\n",
    "        if not chars:\n",
    "            return\n",
    "        level = self.keyword_chains\n",
    "        for i in range(len(chars)):\n",
    "            if chars[i] in level:\n",
    "                level = level[chars[i]]\n",
    "            else:\n",
    "                if not isinstance(level, dict):\n",
    "                    break\n",
    "                for j in range(i, len(chars)):\n",
    "                    level[chars[j]] = {}\n",
    "                    last_level, last_char = level, chars[j]\n",
    "                    level = level[chars[j]]\n",
    "                last_level[last_char] = {self.delimit: 0}\n",
    "                break\n",
    "        if i == len(chars) - 1:\n",
    "            level[self.delimit] = 0\n",
    "\n",
    "    def parse(self, path):\n",
    "        with open(path,encoding='utf-8') as f:\n",
    "            for keyword in f:\n",
    "                self.add(str(keyword).strip())\n",
    "\n",
    "    def filter(self, message,matchType):\n",
    "        message = message.lower()\n",
    "        ret = []\n",
    "        rskey=[]\n",
    "        start = 0\n",
    "        tag=0\n",
    "        while start <len(message)-1:\n",
    "            level = self.keyword_chains\n",
    "            step_ins = 0\n",
    "            for char in message[start:]:\n",
    "                if char in level:\n",
    "                    step_ins += 1\n",
    "                    if self.delimit not in level[char]:\n",
    "                        level = level[char]\n",
    "                    else:\n",
    "                        tag=1\n",
    "                        tem=step_ins\n",
    "                        #start += step_ins - 1\n",
    "                        if \"minMatch\"==matchType:\n",
    "                            rskey.append(message[start:start + step_ins])\n",
    "                            start += step_ins\n",
    "                            tag=0\n",
    "                            break\n",
    "                        level = level[char]\n",
    "                else:\n",
    "                    if tag!=0:\n",
    "                        rskey.append(message[start:start + tem])\n",
    "                        start += step_ins-1\n",
    "                    tag = 0\n",
    "                    start+=1\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(\"ok\")\n",
    "                if tag != 0:\n",
    "                    rskey.append(message[start:start + tem])\n",
    "                start += step_ins - 1\n",
    "\n",
    "                #ret.append(message[start])\n",
    "                # start += 1\n",
    "        return rskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/model/xxx.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m gfw \u001b[38;5;241m=\u001b[39m DFAFilter()\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/model/xxx.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxxx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m rs\u001b[38;5;241m=\u001b[39mgfw\u001b[38;5;241m.\u001b[39mfilter(text,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxMatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mDFAFilter.parse\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mstr\u001b[39m(keyword)\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/model/xxx.txt'"
     ]
    }
   ],
   "source": [
    "gfw = DFAFilter()\n",
    "path = \"../data/model/xxx.txt\"\n",
    "gfw.parse(path)\n",
    "text=\"xxx\"\n",
    "rs=gfw.filter(text,'maxMatch')\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æå–å…³é”®è¯ç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRankç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'é©¬åŒ–è…¾', 'weight': 0.2772961200978784}, {'word': 'è…¾è®¯', 'weight': 0.1626016260162572}, {'word': 'äº”è™', 'weight': 0.1626016260162572}, {'word': 'åŸºé‡‘', 'weight': 0.14489715979786527}, {'word': 'è‚¡ä»½', 'weight': 0.14489715979786527}, {'word': 'æ•°æ®', 'weight': 0.0833160643714195}, {'word': 'æ¦œå•', 'weight': 0.02439024390245698}]\n"
     ]
    }
   ],
   "source": [
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "# å…³é”®è¯æŠ½å–\n",
    "def keywords_extraction(text):\n",
    "    tr4w = TextRank4Keyword(stop_words_file='../data/dict/Stopword.txt',allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])\n",
    "\n",
    "    #allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz']\n",
    "    # allow_speech_tags   --è¯æ€§åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤æŸäº›è¯æ€§çš„è¯\n",
    "    tr4w.analyze(text=text, window=3, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.85, })\n",
    "    # text    --  æ–‡æœ¬å†…å®¹ï¼Œå­—ç¬¦ä¸²\n",
    "    # window  --  çª—å£å¤§å°ï¼Œintï¼Œç”¨æ¥æ„é€ å•è¯ä¹‹é—´çš„è¾¹ã€‚é»˜è®¤å€¼ä¸º2\n",
    "    # lower   --  æ˜¯å¦å°†è‹±æ–‡æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤å€¼ä¸ºFalse\n",
    "    # vertex_source  -- é€‰æ‹©ä½¿ç”¨words_no_filter, words_no_stop_words, words_all_filtersä¸­çš„å“ªä¸€ä¸ªæ¥æ„é€ pagerankå¯¹åº”çš„å›¾ä¸­çš„èŠ‚ç‚¹\n",
    "    #                -- é»˜è®¤å€¼ä¸º`'all_filters'`ï¼Œå¯é€‰å€¼ä¸º`'no_filter', 'no_stop_words', 'all_filters'\n",
    "    # edge_source  -- é€‰æ‹©ä½¿ç”¨words_no_filter, words_no_stop_words, words_all_filtersä¸­çš„å“ªä¸€ä¸ªæ¥æ„é€ pagerankå¯¹åº”çš„å›¾ä¸­çš„èŠ‚ç‚¹ä¹‹é—´çš„è¾¹\n",
    "    #              -- é»˜è®¤å€¼ä¸º`'no_stop_words'`ï¼Œå¯é€‰å€¼ä¸º`'no_filter', 'no_stop_words', 'all_filters'`ã€‚è¾¹çš„æ„é€ è¦ç»“åˆ`window`å‚æ•°\n",
    "\n",
    "    # pagerank_config  -- pagerankç®—æ³•å‚æ•°é…ç½®ï¼Œé˜»å°¼ç³»æ•°ä¸º0.85\n",
    "    keywords = tr4w.get_keywords(num=8, word_min_len=2)\n",
    "    # num           --  è¿”å›å…³é”®è¯æ•°é‡\n",
    "    # word_min_len  --  è¯çš„æœ€å°é•¿åº¦ï¼Œé»˜è®¤å€¼ä¸º1\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# å…³é”®çŸ­è¯­æŠ½å–\n",
    "def keyphrases_extraction(text):\n",
    "    tr4w = TextRank4Keyword(stop_words_file='../data/dict/Stopword.txt',allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])\n",
    "    tr4w.analyze(text=text, window=3, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.35, })\n",
    "    keyphrases = tr4w.get_keyphrases(keywords_num=20, min_occur_num=1)\n",
    "    # keywords_num    --  æŠ½å–çš„å…³é”®è¯æ•°é‡\n",
    "    # min_occur_num   --  å…³é”®çŸ­è¯­åœ¨æ–‡ä¸­çš„æœ€å°‘å‡ºç°æ¬¡æ•°\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "# å…³é”®å¥æŠ½å–\n",
    "def keysentences_extraction(text):\n",
    "    tr4s = TextRank4Sentence()\n",
    "    tr4s.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.85, })\n",
    "    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)\n",
    "    return keysentences\n",
    "\n",
    "text='ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚'\n",
    "rs=keywords_extraction(text)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDFæŠ½å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('é©¬åŒ–è…¾', 2.1735940914363634),\n",
       " ('äº”è™', 1.0377973638363636),\n",
       " ('è…¾è®¯', 0.8269687824763636),\n",
       " ('æ¦œå•', 0.8075816195600001),\n",
       " ('å¥—ç°', 0.7224940288809091),\n",
       " ('æŒè‚¡', 0.5777534456690909),\n",
       " ('ä¸åˆ°', 0.48511084255363635),\n",
       " ('è‚¡ä»½', 0.4397362419445454),\n",
       " ('æ•°æ®', 0.43466878894454547),\n",
       " ('åŸºé‡‘', 0.3660047858290909)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jieba import analyse\n",
    "def getkwTfidf(text):\n",
    "    wordlist=cut_wd(text)\n",
    "    wordstext=\",\".join(wordlist)\n",
    "    tfidf = analyse.extract_tags\n",
    "    keywords = tfidf(wordstext,topK=20,withWeight=True,\n",
    "                     allowPOS=('ns', 'nr', 'nt', 'nz', 'nl', 'n', 'vn', 'vd', 'vg', 'v', 'vf', 'a', 'an', 'i'))\n",
    "    return keywords\n",
    "text='ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚'\n",
    "keywords=getkwTfidf(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## èšç±»ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of emb is ===== 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚è…¾è®¯çš„å¤§è‚¡ä¸œæ˜¯å—éæŠ¥ä¸šé›†å›¢ã€‚',\n",
       "  'å¤§è‚¡ä¸œNaspersæ˜¯ä¸€å®¶1915å¹´æˆç«‹ï¼Œæ€»éƒ¨ä½äºå—éçš„ä¼ åª’é›†å›¢ã€‚2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼ã€‚',\n",
       "  'ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚'],\n",
       " ['é˜¿é‡Œå·´å·´è¿™å®¶å…¬å¸æ—©å¹´ä¹Ÿå·®é’±ï¼Œé©¬äº‘çš„é˜¿é‡Œè·å¾—äº†å­™æ­£ä¹‰è½¯é“¶çš„2000ä¸‡ç¾å…ƒæŠ•èµ„ã€‚è‡ªæ­¤é˜¿é‡Œå·´å·´çš„B2Bä¸šåŠ¡å¼€å§‹è¿›å†›å›½é™…å¸‚åœºï¼Œå¹¶åœ¨å›½é™…å¸‚åœºä¸Šæ‰“å‡ºäº†åæ°”ã€‚'],\n",
       " ['ç½‘å‹ä»¬éƒ½æ¸…æ¥šï¼Œä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸šï¼Œä¸€æ–¹é¢æ˜¯å¾—ç›Šäºä¸­å›½äº’è”ç½‘åœ¨å•†ä¸šæ¨¡å¼ä¸Šçš„åˆ›æ–°ï¼Œå¦ä¸€æ–¹é¢åˆ™æ˜¯å—ç›Šäºèµ„æœ¬çš„é©±åŠ¨ã€‚']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#æ–‡æœ¬èšç±»å¥å‘é‡ï¼Œç„¶åå†é€šè¿‡kmeansç­‰æ–¹æ³•è¿›è¡Œèšç±»\n",
    "def getkmeans(corpus):\n",
    "    corpus_embeddings = embedder.encode(corpus)\n",
    "    print(\"len of emb is =====\", len(corpus_embeddings))\n",
    "    num_clusters = 3\n",
    "    clustering_model = KMeans(n_clusters=num_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    clustered_sentences = [[] for i in range(num_clusters)]\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "    return clustered_sentences\n",
    "corpus = ['ç½‘å‹ä»¬éƒ½æ¸…æ¥šï¼Œä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸šï¼Œä¸€æ–¹é¢æ˜¯å¾—ç›Šäºä¸­å›½äº’è”ç½‘åœ¨å•†ä¸šæ¨¡å¼ä¸Šçš„åˆ›æ–°ï¼Œå¦ä¸€æ–¹é¢åˆ™æ˜¯å—ç›Šäºèµ„æœ¬çš„é©±åŠ¨ã€‚',\n",
    "'é˜¿é‡Œå·´å·´è¿™å®¶å…¬å¸æ—©å¹´ä¹Ÿå·®é’±ï¼Œé©¬äº‘çš„é˜¿é‡Œè·å¾—äº†å­™æ­£ä¹‰è½¯é“¶çš„2000ä¸‡ç¾å…ƒæŠ•èµ„ã€‚è‡ªæ­¤é˜¿é‡Œå·´å·´çš„B2Bä¸šåŠ¡å¼€å§‹è¿›å†›å›½é™…å¸‚åœºï¼Œå¹¶åœ¨å›½é™…å¸‚åœºä¸Šæ‰“å‡ºäº†åæ°”ã€‚',\n",
    "'åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚è…¾è®¯çš„å¤§è‚¡ä¸œæ˜¯å—éæŠ¥ä¸šé›†å›¢ã€‚',\n",
    "'å¤§è‚¡ä¸œNaspersæ˜¯ä¸€å®¶1915å¹´æˆç«‹ï¼Œæ€»éƒ¨ä½äºå—éçš„ä¼ åª’é›†å›¢ã€‚2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼ã€‚',\n",
    "'ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚',]\n",
    "cluster=getkmeans(corpus)\n",
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................\n",
      "æ–‡æœ¬å·²ç»åˆ†è¯å®Œæ¯• !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................\n",
      "å¾—åˆ°çš„ä¸»é¢˜æ•°é‡æœ‰: 1 ä¸ª ...\n",
      "............................................................................................\n",
      "\n",
      "ã€ä¸»é¢˜ç´¢å¼•ã€‘:0 \n",
      "ã€ä¸»é¢˜è¯­é‡ã€‘ï¼š5 \n",
      "ã€ä¸»é¢˜å…³é”®è¯ã€‘ï¼šè…¾è®¯,è‚¡ä»½,æ—©æœŸ,èµ„æœ¬,ä¸šåŠ¡,è¡Œä¸š,å¸‚å€¼,å—é,naspers,å•†ä¸šæ¨¡å¼,å¾—ç›Šäº,ä¸åˆ°,å…¬å¸,äº’è”ç½‘,åˆ›æ–°,ä¸Šå¸‚å…¬å¸,ä¼ åª’,æŒè‚¡,å­™æ­£ä¹‰,å·®é’± \n",
      "ã€ä¸»é¢˜ä¸­å¿ƒå¥ã€‘ ï¼š\n",
      "2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼\n",
      "ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­\n",
      "åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# singlePassç®—æ³•ï¼Œé€‚åˆè¾ƒå¤§è¯­æ–™åœºæ™¯\n",
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "import json\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from smart_open import  smart_open\n",
    "import pandas as pd\n",
    "from  textrank4zh import TextRank4Keyword,TextRank4Sentence #å…³é”®è¯å’Œå…³é”®å¥æå–\n",
    "from tkinter import _flatten   #ç”¨äºå°†åµŒå¥—åˆ—è¡¨å‹æˆä¸€å±‚\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy\n",
    "\n",
    "class Single_Pass_Cluster(object):\n",
    "    def __init__(self,\n",
    "                 filename,\n",
    "                 corpus,\n",
    "                 stop_words_file= '../data/dict/Stopword.txt',\n",
    "                 theta = 0.5):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.stop_words_file = stop_words_file\n",
    "        self.theta = theta\n",
    "        self.corpus = corpus\n",
    "\n",
    "    #'''ä»¥åˆ—è¡¨çš„å½¢å¼è¯»å–æ–‡æ¡£'''\n",
    "    def loadData(self,filename):\n",
    "\n",
    "        Data = []\n",
    "        i = 0\n",
    "        if filename==None:\n",
    "            Data= None\n",
    "        else:\n",
    "            \n",
    "            with smart_open(self.filename,encoding='utf-8') as f:\n",
    "                tick=f.readlines()[1:10001]\n",
    "                #é‰´äºæœ‰äº›æ–‡æ¡£è¾ƒé•¿ï¼ŒåŒ…å«å¤šä¸ªè¯­ä¹‰ä¸­å¿ƒï¼Œå› æ­¤æŒ‰è¯­å¥ç»“æŸæ ‡ç‚¹è¿›è¡Œåˆ‡å‰²è·å–è¡¨æ„å•ä¸€çš„å¥å­äº§ç”Ÿçš„èšç±»æ•ˆæœä¼šæ›´å¥½\n",
    "                texts = [cut_sentences(i) for i in tick]\n",
    "                print('æœªåˆ‡å‰²å‰çš„è¯­å¥æ€»æ•°æœ‰{}æ¡...'.format(len(texts)))\n",
    "                print (\"............................................................................................\")\n",
    "                texts = [i.strip() for i in list(_flatten(texts)) if len(i)>5]\n",
    "                print('åˆ‡å‰²åçš„è¯­å¥æ€»æ•°æœ‰{}æ¡...'.format(len(texts)))\n",
    "                for line in texts:\n",
    "                    i  += 1\n",
    "                    Data.append(line )\n",
    "        return Data\n",
    "\n",
    "    def word_segment(self,texts):\n",
    "    #'''å¯¹è¯­å¥è¿›è¡Œåˆ†è¯ï¼Œå¹¶å»æ‰å¸¸è§æ— æ„ä¹‰çš„é«˜é¢‘è¯ï¼ˆåœç”¨è¯ï¼‰'''\n",
    "        stopwords = [line.strip() for line in open( self.stop_words_file,encoding='utf-8').readlines()]\n",
    "        word_segmentation = []\n",
    "        words = jieba.cut(texts)\n",
    "        for word in words:\n",
    "            if word == ' ':\n",
    "                continue\n",
    "            if word not in stopwords and word.isdigit()==False:\n",
    "                word_segmentation.append(word)\n",
    "        return word_segmentation\n",
    "\n",
    "    def get_Tfidf_vector_representation(self,word_segmentation,pivot= 10, slope = 0.1):\n",
    "        #'''é‡‡ç”¨VSM(vector space model)å¾—åˆ°æ–‡æ¡£çš„ç©ºé—´å‘é‡è¡¨ç¤ºï¼Œä¹Ÿå¯ä»¥doc2vecç­‰ç®—æ³•ç›´æ¥è·å–å¥å‘é‡'''\n",
    "\n",
    "        dictionary = corpora.Dictionary(word_segmentation)  #è·å–åˆ†è¯åè¯æ±‡å’Œè¯æ±‡idçš„æ˜ å°„å…³ç³»ï¼Œå½¢æˆå­—å…¸\n",
    "        corpus = [dictionary.doc2bow(text) for text in word_segmentation]   #å¾—åˆ°è¯­å¥çš„å‘é‡è¡¨ç¤º\n",
    "        tfidf = models.TfidfModel(corpus,pivot=pivot, slope =slope)      #è¿›ä¸€æ­¥è·å–è¯­å¥çš„TF-IDFå‘é‡è¡¨ç¤º\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        return corpus_tfidf\n",
    "\n",
    "    def get_bert_vector(self,word_segmentation):\n",
    "        embedder = SentenceTransformer('hfl/chinese-electra-180g-small-generator')\n",
    "        corpus_embeddings=embedder.encode(word_segmentation)\n",
    "        return corpus_embeddings\n",
    "\n",
    "    def getMaxSimilarity(self,dictTopic, vector):\n",
    "     #  '''è®¡ç®—æ–°è¿›å…¥æ–‡æ¡£å’Œå·²æœ‰æ–‡æ¡£çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œè¿™é‡Œçš„ç›¸ä¼¼åº¦é‡‡ç”¨çš„æ˜¯cosineä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¤§å®¶è¿˜å¯ä»¥è¯•è¯•\n",
    "    #kullback_leibler, jaccard, hellingerç­‰ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•'''\n",
    "\n",
    "        maxValue = 0\n",
    "        maxIndex = -1\n",
    "        for k,cluster in dictTopic.items():\n",
    "            oneSimilarity = np.mean([matutils.cossim(vector, v) for v in cluster])\n",
    "            if oneSimilarity > maxValue:\n",
    "                maxValue = oneSimilarity\n",
    "                maxIndex = k\n",
    "        return maxIndex, maxValue\n",
    "\n",
    "    def gettopKsim(query_embedding, sentence_embeddings):\n",
    "        number_top_matches = 3\n",
    "        senindexrs = []\n",
    "        topkrs = {}\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "        results = zip(range(1, len(distances) + 1), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        # print(\"Query:\", query)\n",
    "        # print(\"\\nTop {} most similar sentences in corpus:\".format(number_top_matches))\n",
    "\n",
    "        for idx, distance in results[0:number_top_matches]:\n",
    "            # print(\"index\",idx, \"(Cosine Score: %.4f)\" % (1 - distance))\n",
    "            topkrs[idx] = 1 - distance\n",
    "            senindexrs.append(idx)\n",
    "        print(\"å€™é€‰ç­”æ¡ˆï¼š\", topkrs)\n",
    "        return topkrs\n",
    "\n",
    "\n",
    "\n",
    "    def getMaxSimbert(self,dictTopic, vector):\n",
    "     #  '''è®¡ç®—æ–°è¿›å…¥æ–‡æ¡£å’Œå·²æœ‰æ–‡æ¡£çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œè¿™é‡Œçš„ç›¸ä¼¼åº¦é‡‡ç”¨çš„æ˜¯cosineä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ ¹æ®ä¸Šä¸€ç¯‡æ–‡ç« çš„æç¤ºï¼Œå¤§å®¶è¿˜å¯ä»¥è¯•è¯•\n",
    "    #kullback_leibler, jaccard, hellingerç­‰ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•'''\n",
    "\n",
    "        maxValue = 0\n",
    "        maxIndex = -1\n",
    "        for k,cluster in dictTopic.items():\n",
    "            Similaritys =scipy.spatial.distance.cdist([vector], cluster, \"cosine\")[0]\n",
    "\n",
    "            results = sorted(Similaritys)\n",
    "            oneSimilarity=1-results[0]\n",
    "            if oneSimilarity > maxValue:\n",
    "                maxValue = oneSimilarity\n",
    "                maxIndex = k\n",
    "        return maxIndex, maxValue\n",
    "\n",
    "    def single_pass(self,corpus,texts,theta):\n",
    "        dictTopic = {}\n",
    "        clusterTopic = {}\n",
    "        numTopic = 0\n",
    "        cnt = 0\n",
    "        for vector,text in zip(corpus,texts):\n",
    "            if numTopic == 0:\n",
    "                dictTopic[numTopic] = []\n",
    "                dictTopic[numTopic].append(vector)\n",
    "                clusterTopic[numTopic] = []\n",
    "                clusterTopic[numTopic].append(text)\n",
    "                numTopic += 1\n",
    "            else:\n",
    "                #maxIndex, maxValue = self.getMaxSimilarity(dictTopic, vector)\n",
    "                maxIndex, maxValue = self.getMaxSimbert(dictTopic, vector)\n",
    "                # ä»¥ç¬¬ä¸€ç¯‡æ–‡æ¡£ä¸ºç§å­ï¼Œå»ºç«‹ä¸€ä¸ªä¸»é¢˜ï¼Œå°†ç»™å®šè¯­å¥åˆ†é…åˆ°ç°æœ‰çš„ã€æœ€ç›¸ä¼¼çš„ä¸»é¢˜ä¸­\n",
    "                if maxValue > theta:\n",
    "                    dictTopic[maxIndex].append(vector)\n",
    "                    clusterTopic[maxIndex].append(text)\n",
    "\n",
    "                # æˆ–è€…åˆ›å»ºä¸€ä¸ªæ–°çš„ä¸»é¢˜\n",
    "                else:\n",
    "                    dictTopic[numTopic] = []\n",
    "                    dictTopic[numTopic].append(vector)\n",
    "                    clusterTopic[numTopic] = []\n",
    "                    clusterTopic[numTopic].append(text)\n",
    "                    numTopic += 1\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print (\"processing {}...\".format(cnt))\n",
    "        return dictTopic, clusterTopic\n",
    "\n",
    "    def fit_transform(self,theta=0.5):\n",
    "\n",
    "     # '''ç»¼åˆä¸Šè¿°çš„å‡½æ•°ï¼Œå¾—å‡ºæœ€ç»ˆçš„èšç±»ç»“æœï¼šåŒ…æ‹¬èšç±»çš„æ ‡å·ã€æ¯ä¸ªèšç±»çš„æ•°é‡ã€å…³é”®ä¸»é¢˜è¯å’Œå…³é”®è¯­å¥'''\n",
    "        datMat = self.loadData(self.filename)\n",
    "        if datMat==None:\n",
    "            datMat=self.corpus\n",
    "        word_segmentation = []\n",
    "        for i in range(len(datMat)):\n",
    "            wdlist=self.word_segment(datMat[i])\n",
    "            word_segmentation.append(\"\".join(wdlist))\n",
    "\n",
    "            #word_segmentation.append(wdlist)\n",
    "        print (\"............................................................................................\")\n",
    "        print('æ–‡æœ¬å·²ç»åˆ†è¯å®Œæ¯• !')\n",
    "\n",
    "        #å¾—åˆ°æ–‡æœ¬æ•°æ®çš„ç©ºé—´å‘é‡è¡¨ç¤º\n",
    "        #corpus_tfidf = self.get_Tfidf_vector_representation(word_segmentation)\n",
    "        corpus_bert = self.get_bert_vector(word_segmentation)\n",
    "        dictTopic, clusterTopic = self.single_pass(corpus_bert, datMat, theta)\n",
    "        print (\"............................................................................................\")\n",
    "        print( \"å¾—åˆ°çš„ä¸»é¢˜æ•°é‡æœ‰: {} ä¸ª ...\".format(len(dictTopic)))\n",
    "        print (\"............................................................................................\\n\")\n",
    "        #æŒ‰èšç±»è¯­å¥æ•°é‡å¯¹èšç±»ç»“æœè¿›è¡Œé™åºæ’åˆ—ï¼Œæ‰¾åˆ°é‡è¦çš„èšç±»ç¾¤\n",
    "        clusterTopic_list = sorted(clusterTopic.items(),key=lambda x: len(x[1]),reverse=True)\n",
    "        # f=open(\"../data/cluster_topic.txt\",\"w\",encoding=\"utf-8\")\n",
    "        for k in clusterTopic_list:\n",
    "            cluster_title = '\\n'.join(k[1])\n",
    "            # å¾—åˆ°æ¯ä¸ªèšç±»ä¸­çš„ä¸»é¢˜å…³é”®è¯\n",
    "            word = TextRank4Keyword()\n",
    "            word.analyze(''.join(self.word_segment(''.join(cluster_title))),window = 5,lower = True)\n",
    "            w_list = word.get_keywords(num = 20,word_min_len = 2)\n",
    "           # å¾—åˆ°æ¯ä¸ªèšç±»ä¸­çš„å…³é”®ä¸»é¢˜å¥TOP3\n",
    "            sentence = TextRank4Sentence()\n",
    "            sentence.analyze(' '.join(k[1]) ,lower = True)\n",
    "            s_list = sentence.get_key_sentences(num = 3,sentence_min_len = 3)\n",
    "            clustcenter=','.join([i.word for i in w_list])+'||'.join([i.sentence for i in s_list])\n",
    "            wrcontent=\"ã€ä¸»é¢˜ç´¢å¼•ã€‘:{} \\nã€ä¸»é¢˜è¯­é‡ã€‘ï¼š{} \\nã€ä¸»é¢˜å…³é”®è¯ã€‘ï¼š{} \\nã€ä¸»é¢˜ä¸­å¿ƒå¥ã€‘ ï¼š\\n{}\".format(k[0], len(k[1]),\n",
    "                                                                              ','.join([i.word for i in w_list]),\n",
    "                                                                              '\\n'.join([i.sentence for i in s_list]))\n",
    "            # f.write(wrcontent+\"\\n\")\n",
    "            print(\"ã€ä¸»é¢˜ç´¢å¼•ã€‘:{} \\nã€ä¸»é¢˜è¯­é‡ã€‘ï¼š{} \\nã€ä¸»é¢˜å…³é”®è¯ã€‘ï¼š{} \\nã€ä¸»é¢˜ä¸­å¿ƒå¥ã€‘ ï¼š\\n{}\".format(k[0], len(k[1]),\n",
    "                                                                              ','.join([i.word for i in w_list]),\n",
    "                                                                              '\\n'.join([i.sentence for i in s_list])))\n",
    "            print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = ['ç½‘å‹ä»¬éƒ½æ¸…æ¥šï¼Œä¸­å›½äº’è”ç½‘è¡Œä¸šä¹‹æ‰€ä»¥èƒ½åª²ç¾ç¾å›½äº’è”ç½‘è¡Œä¸šï¼Œä¸€æ–¹é¢æ˜¯å¾—ç›Šäºä¸­å›½äº’è”ç½‘åœ¨å•†ä¸šæ¨¡å¼ä¸Šçš„åˆ›æ–°ï¼Œå¦ä¸€æ–¹é¢åˆ™æ˜¯å—ç›Šäºèµ„æœ¬çš„é©±åŠ¨ã€‚',\n",
    "'é˜¿é‡Œå·´å·´è¿™å®¶å…¬å¸æ—©å¹´ä¹Ÿå·®é’±ï¼Œé©¬äº‘çš„é˜¿é‡Œè·å¾—äº†å­™æ­£ä¹‰è½¯é“¶çš„2000ä¸‡ç¾å…ƒæŠ•èµ„ã€‚è‡ªæ­¤é˜¿é‡Œå·´å·´çš„B2Bä¸šåŠ¡å¼€å§‹è¿›å†›å›½é™…å¸‚åœºï¼Œå¹¶åœ¨å›½é™…å¸‚åœºä¸Šæ‰“å‡ºäº†åæ°”ã€‚',\n",
    "'åŒæ ·çš„ï¼Œè…¾è®¯å…¬å¸åœ¨æ—©æœŸä¹Ÿæ˜¯è·å¾—äº†èµ„æœ¬çš„æ”¯æŒï¼Œæ‰å–å¾—äº†å¿«é€Ÿå‘å±•ã€‚è…¾è®¯çš„å¤§è‚¡ä¸œæ˜¯å—éæŠ¥ä¸šé›†å›¢ã€‚',\n",
    "'å¤§è‚¡ä¸œNaspersæ˜¯ä¸€å®¶1915å¹´æˆç«‹ï¼Œæ€»éƒ¨ä½äºå—éçš„ä¼ åª’é›†å›¢ã€‚2001å¹´è´­ä¹°äº†è…¾è®¯çš„è‚¡ä»½ï¼Œéšç€è…¾è®¯å¸‚å€¼é£™æ¶¨ï¼ŒNaspersæˆä¸ºæ•´ä¸ªéæ´²å¸‚å€¼æœ€å¤§çš„ä¸Šå¸‚å…¬å¸ï¼Œå…¶è…¾è®¯çš„è‚¡æƒå¸‚å€¼ç”šè‡³è¶…è¿‡äº†è‡ªèº«ä¸šåŠ¡çš„å¸‚å€¼ã€‚',\n",
    "'ä»ä»¥ä¸Šæ•°æ®å¯ä»¥çœ‹å‡ºï¼Œé©¬åŒ–è…¾åŠ ä¸Šé©¬åŒ–è…¾åŸºé‡‘çš„è‚¡ä»½ï¼Œä¹ŸæŒè‚¡ä¸åˆ°9%ï¼Œåƒè…¾è®¯æ—©æœŸçš„äº”è™ï¼Œå¤§å¤šä¹Ÿå¥—ç°ä¸å°‘ï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨10å¤§è‚¡ä¸œçš„æ¦œå•ä¹‹ä¸­ã€‚',]\n",
    "    single_pass_cluster = Single_Pass_Cluster(filename=None,corpus=corpus, stop_words_file='../data/dict/Stopword.txt')\n",
    "    single_pass_cluster.fit_transform(theta=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†ç±»ç®—æ³•ï¼ˆbert+finetuneï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110k/110k [00:00<00:00, 199kB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<00:00, 6.14kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624/624 [00:00<00:00, 159kB/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/zhangguoqiang/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "    9 è®­ç»ƒæ•°æ®\n",
      "    1 éªŒè¯æ•°æ®\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 412M/412M [00:48<00:00, 8.40MB/s] \n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/zhangguoqiang/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (21128, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                          (12, 768)\n",
      "classifier.bias                                                (12,)\n",
      "Epoch 1 / 2\n",
      "è®­ç»ƒå‡†ç¡®ç‡: 0.33\n",
      "å¹³å‡è®­ç»ƒæŸå¤± loss: 2.21\n",
      "è®­ç»ƒæ—¶é—´: 0:00:03\n",
      "\n",
      "æµ‹è¯•å‡†ç¡®ç‡: 0.00\n",
      "å¹³å‡æµ‹è¯•æŸå¤± Loss: 2.43\n",
      "æµ‹è¯•æ—¶é—´: 0:00:00\n",
      "Epoch 2 / 2\n",
      "è®­ç»ƒå‡†ç¡®ç‡: 0.56\n",
      "å¹³å‡è®­ç»ƒæŸå¤± loss: 2.03\n",
      "è®­ç»ƒæ—¶é—´: 0:00:03\n",
      "\n",
      "æµ‹è¯•å‡†ç¡®ç‡: 0.00\n",
      "å¹³å‡æµ‹è¯•æŸå¤± Loss: 2.38\n",
      "æµ‹è¯•æ—¶é—´: 0:00:00\n",
      "è®­ç»ƒä¸€å…±ç”¨äº† 0:00:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# è°ƒç”¨çš„bertæ¨¡å‹åç§°ï¼›\n",
    "# æ„é€ è®­ç»ƒæ•°æ®é›†\n",
    "# æ„é€ input tokenï¼Œmaskç­‰å¤„ç†\n",
    "# è¶…å‚æ•°è°ƒæ•´ä¼˜åŒ–max_lengthï¼Œbatch_sizeï¼Œepochsï¼Œlr\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import time,datetime\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # è¿”å› hh:mm:ss å½¢å¼çš„æ—¶é—´\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "print('Download BERT tokenizer...')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    print('There are %d GPU(s) available.' % n_gpu)\n",
    "\n",
    "    print('We will use the GPU:', [torch.cuda.get_device_name(i) for i in range(n_gpu)])\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_data = pd.read_csv(\"../data/bert_train_data/train.csv\", sep='\\t')\n",
    "desclist=train_data['desc'].unique().tolist()\n",
    "train_data['label']=train_data['desc'].apply(lambda x : desclist.index(x))\n",
    "test_data = pd.read_csv(\"../data/bert_train_data/test.csv\", sep='\\t')\n",
    "\n",
    "# å¾ªç¯æ¯ä¸€ä¸ªå¥å­...\n",
    "sentences = train_data['content'].tolist()\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=256,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # æŠŠç¼–ç çš„å¥å­åŠ å…¥list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # åŠ ä¸Š attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# æŠŠlists è½¬ä¸º tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = train_data['label'].values\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "output_dir = \"../data/bert_train_data/models/\"\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "# ä»£ç å‚è€ƒ https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­.\n",
    "\n",
    "# æŠŠinput æ”¾å…¥ TensorDatasetã€‚\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# è®¡ç®— train_size å’Œ val_size çš„é•¿åº¦.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# 90% çš„dataset ä¸ºtrain_dataset, 10% çš„çš„dataset ä¸ºval_dataset.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} è®­ç»ƒæ•°æ®'.format(train_size))\n",
    "print('{:>5,} éªŒè¯æ•°æ®'.format(val_size))\n",
    "\n",
    "# æ¨èbatch_size ä¸º 16 æˆ–è€… 32\n",
    "batch_size = 16\n",
    "\n",
    "# ä¸ºè®­ç»ƒæ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†è®¾è®¡DataLoaders.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # è®­ç»ƒæ•°æ®.\n",
    "            sampler = RandomSampler(train_dataset), # æ‰“ä¹±é¡ºåº\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # éªŒè¯æ•°æ®.\n",
    "            sampler = RandomSampler(val_dataset), # æ‰“ä¹±é¡ºåº\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\", # ä½¿ç”¨ 12-layer çš„ BERT æ¨¡å‹.\n",
    "    num_labels = 12, # å¤šåˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºæ ‡ç­¾ä¸º 12ä¸ª.\n",
    "    output_attentions = False, # ä¸è¿”å› attentions weights.\n",
    "    output_hidden_states = False, # ä¸è¿”å› all hidden-states.\n",
    ")\n",
    "model.cpu()\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# AdamW æ˜¯ä¸€ä¸ª huggingface library çš„ç±»ï¼Œ'W' æ˜¯'Weight Decay fix\"çš„æ„æ€ã€‚\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - é»˜è®¤æ˜¯ 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - é»˜è®¤æ˜¯ 1e-8ï¼Œ æ˜¯ä¸ºäº†é˜²æ­¢è¡°å‡ç‡åˆ†æ¯é™¤åˆ°0\n",
    "                )\n",
    "\n",
    "# bert æ¨è epochs åœ¨2åˆ°4ä¹‹é—´ä¸ºå¥½ã€‚\n",
    "epochs = 2\n",
    "\n",
    "# training steps çš„æ•°é‡: [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# è®¾è®¡ learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "seed_val = 2021\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "# torch.manual_seed_all(seed_val)\n",
    "\n",
    "# è®°å½•training ,validation loss ,validation accuracy and timings.\n",
    "training_stats = []\n",
    "\n",
    "# è®¾ç½®æ€»æ—¶é—´.\n",
    "total_t0 = time.time()\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    # è®°å½•æ¯ä¸ª epoch æ‰€ç”¨çš„æ—¶é—´\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # æ¯éš”40ä¸ªbatch è¾“å‡ºä¸€ä¸‹æ‰€ç”¨æ—¶é—´.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # `batch` åŒ…æ‹¬3ä¸ª tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # æ¸…ç©ºæ¢¯åº¦\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # å‚è€ƒ https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        pred = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "        logits=pred.logits\n",
    "        loss=pred.loss\n",
    "        #print(loss, logits)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # backward æ›´æ–° gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # å‡å»å¤§äº1 çš„æ¢¯åº¦ï¼Œå°†å…¶è®¾ä¸º 1.0, ä»¥é˜²æ¢¯åº¦çˆ†ç‚¸.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        optimizer.step()\n",
    "\n",
    "        # æ›´æ–° learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        logit = logits.detach().cpu().numpy()\n",
    "        label_id = b_labels.to('cpu').numpy()\n",
    "        # è®¡ç®—training å¥å­çš„å‡†ç¡®åº¦.\n",
    "        total_train_accuracy += flat_accuracy(logit, label_id)\n",
    "\n",
    "        # è®¡ç®—batchesçš„å¹³å‡æŸå¤±.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    # è®­ç»ƒé›†çš„å‡†ç¡®ç‡.\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    print(\"è®­ç»ƒå‡†ç¡®ç‡: {0:.2f}\".format(avg_train_accuracy))\n",
    "    print(\"å¹³å‡è®­ç»ƒæŸå¤± loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"è®­ç»ƒæ—¶é—´: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # è®¾ç½® model ä¸ºvaluation çŠ¶æ€ï¼Œåœ¨valuationçŠ¶æ€ dropout layers çš„dropout rateä¼šä¸åŒ\n",
    "    model.eval()\n",
    "\n",
    "    # è®¾ç½®å‚æ•°\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # `batch` åŒ…æ‹¬3ä¸ª tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # åœ¨valuation çŠ¶æ€ï¼Œä¸æ›´æ–°æƒå€¼ï¼Œä¸æ”¹å˜è®¡ç®—å›¾\n",
    "        with torch.no_grad():\n",
    "            # å‚è€ƒ https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            pred = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            logits = pred.logits\n",
    "            loss = pred.loss\n",
    "\n",
    "        # è®¡ç®— validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        logit = logits.detach().cpu().numpy()\n",
    "        label_id = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # è®¡ç®— validation å¥å­çš„å‡†ç¡®åº¦.\n",
    "        total_eval_accuracy += flat_accuracy(logit, label_id)\n",
    "\n",
    "    # è®¡ç®— validation çš„å‡†ç¡®ç‡.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"\")\n",
    "    print(\"æµ‹è¯•å‡†ç¡®ç‡: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        torch.save(model.state_dict(), output_model_file)\n",
    "        model.config.to_json_file(output_config_file)\n",
    "        tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "    # è®¡ç®—batchesçš„å¹³å‡æŸå¤±.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # è®¡ç®—validation æ—¶é—´.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"å¹³å‡æµ‹è¯•æŸå¤± Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"æµ‹è¯•æ—¶é—´: {:}\".format(validation_time))\n",
    "\n",
    "    # è®°å½•æ¨¡å‹å‚æ•°\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"è®­ç»ƒä¸€å…±ç”¨äº† {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:49:34.494983Z",
     "start_time": "2021-12-24T08:49:27.550022Z"
    }
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../data/bert_train_data/models'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m loadstart\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# è¯»å–æ¨¡å‹å¯¹åº”çš„tokenizer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/bert_train_data/models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# è½½å…¥æ¨¡å‹\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/bert_train_data/models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1734\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/utils/hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    405\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/file_download.py:1022\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m REPO_TYPES:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Accepted repo types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(REPO_TYPES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1022\u001b[0m     cache_dir, \u001b[43mrepo_folder_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(storage_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# cross platform transcription of filename, to be used as a local file path.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/utils/_validators.py:92\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m     90\u001b[0m ):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/utils/_validators.py:136\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../data/bert_train_data/models'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "import time,datetime\n",
    "\n",
    "loadstart=time.time()\n",
    "# è¯»å–æ¨¡å‹å¯¹åº”çš„tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../data/bert_train_data/models')\n",
    "# è½½å…¥æ¨¡å‹\n",
    "model = BertForSequenceClassification.from_pretrained('../data/bert_train_data/models')\n",
    "model = model.cpu()\n",
    "classifier = transformers.pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "loadend=time.time()\n",
    "print(\"load time is ====\",loadend-loadstart)\n",
    "classstart=time.time()\n",
    "texttest=''\n",
    "result=classifier(texttest)\n",
    "label2name={\"0\":\"ç©ºé—´\",\"1\":\"å¤–è§‚\"}\n",
    "labelval=result[0].get(\"label\")\n",
    "lab=labelval.split(\"_\")[1]\n",
    "rs=label2name.get(lab)\n",
    "classend=time.time()\n",
    "print(rs,\"classify time is====\",classend-classstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
