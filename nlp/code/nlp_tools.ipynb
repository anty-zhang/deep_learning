{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP算法工具集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# tfidf\n",
    "# LDA\n",
    "# textrank\n",
    "# word2vec\n",
    "# doc2vec\n",
    "# bert\n",
    "# s-bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "#根据文本的特征，确定需要处理的一些特殊符号\n",
    "def preprocess4wb(text_content):\n",
    "\n",
    "    # 过滤文本中的html链接等\n",
    "    re_tag = re.compile('</?\\w+[^>]*>')  # HTML标签\n",
    "    new_text = re.sub(re_tag, '', text_content)\n",
    "    new_text = re.sub(u\"\\\\[.*?]\", \"\", new_text) #去除[]及其内部内容（表情等会是这样）\n",
    "    new_text = emoji.replace_emoji(new_text,'')  # 去除 emoji\n",
    "    new_text = re.sub(\",+\", \",\", new_text)  # 合并逗号\n",
    "    new_text = re.sub(\" +\", \" \", new_text)  # 合并空格\n",
    "    new_text = re.sub(\"[...|…|。。。]+\", \"...\", new_text)  # 合并句号\n",
    "    new_text = re.sub(\"-+\", \"--\", new_text)  # 合并-\n",
    "    text_content = re.sub(\"———+\", \"———\", new_text)  # 合并-\n",
    "    topic = re.findall('#[^#]+#', str(text_content))#去除话题标识\n",
    "    name = re.findall('@[\\\\u4e00-\\\\u9fa5\\\\w\\\\-]+', str(text_content)) #去除艾特\n",
    "    if topic!=None:\n",
    "        for tp in topic:\n",
    "            text_content=text_content.replace(tp,\"\")\n",
    "    if name!=None:\n",
    "        for nm in name:\n",
    "            text_content=text_content.replace(nm,\"\")\n",
    "    text_content=text_content.replace(\" \",\"\")\n",
    "    punctuation = '~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——+-=“：’；、。，？》《{}「」【】'\n",
    "    text_content = re.sub(r\"[%s]+\" %punctuation, \"\",text_content)\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一个测试文本'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='？「」[asda]123这是一个测...--..。。。。试文本<fffff> #话题# @23海贼王🙃😍😅😘😒 '\n",
    "rs=preprocess4wb(text)\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/cv/39t5r19s6s94k9_p328_hnv00000gp/T/jieba.cache\n",
      "Loading model cost 0.324 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['本文', '详细介绍', '生成', '对抗', '网络', ' ', '–', ' ', 'GAN', ' ', '设计', '初衷', '基本原理', '10', '典型', '算法', '13', '实际应用']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "load_dict_flag = False\n",
    "def load_dict():\n",
    "    # 加载词典\n",
    "    if not load_dict_flag:\n",
    "        jieba.load_userdict(\"../data/dict/SogouLabDic.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_car.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_baidu_utf8.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_pangu.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_sougou_utf8.txt\")\n",
    "        jieba.load_userdict(\"../data/dict/dict_tencent_utf8.txt\")\n",
    "\n",
    "\n",
    "def cut_wd(content):\n",
    "    load_dict()\n",
    "    stopwords = {}.fromkeys([line.rstrip() for line in open('../data/dict/Stopword.txt',encoding='utf-8')])\n",
    "    seg = jieba.cut(content)\n",
    "    wordslist=[]\n",
    "    for i in seg:\n",
    "        if i not in stopwords:\n",
    "            wordslist.append(i)\n",
    "#     wordstext=\" \".join(wordslist)\n",
    "    return wordslist\n",
    "text='本文将详细介绍生成对抗网络 – GAN 的设计初衷、基本原理、10种典型算法和13种实际应用'\n",
    "wdlist=cut_wd(text)\n",
    "print(wdlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分句算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['本文将详细介绍生成对抗网络 – GAN 的设计初衷、', '基本原理、', '10种典型算法和13种实际应用']\n"
     ]
    }
   ],
   "source": [
    "def cut_sentences(content):\n",
    "    # 结束符号，包含中文和英文的,依据具体的需求设定划分依据\n",
    "    end_flag = ['?', '!', '.', '？', '！', '。',';', '…', '、']\n",
    "\n",
    "    content_len = len(content)\n",
    "    sentences = []\n",
    "    tmp_char = ''\n",
    "    for idx, char in enumerate(content):\n",
    "        # 拼接字符\n",
    "        tmp_char += char\n",
    "\n",
    "        # 判断是否已经到了最后一位\n",
    "        if (idx + 1) == content_len:\n",
    "            sentences.append(tmp_char)\n",
    "            break\n",
    "\n",
    "        # 判断此字符是否为结束符号\n",
    "        if char in end_flag:\n",
    "            # 再判断下一个字符是否为结束符号，如果不是结束符号，则切分句子\n",
    "            next_idx = idx + 1\n",
    "            if not content[next_idx] in end_flag:\n",
    "                sentences.append(tmp_char)\n",
    "                tmp_char = ''\n",
    "\n",
    "    return sentences\n",
    "\n",
    "text='本文将详细介绍生成对抗网络 – GAN 的设计初衷、基本原理、10种典型算法和13种实际应用'\n",
    "rs=cut_sentences(text)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体提取-词性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T01:59:07.073273Z",
     "start_time": "2021-12-17T01:59:06.990590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns -> 美国\n",
      "nr -> 马云\n",
      "nr -> 孙正义\n",
      "ns -> 南非\n",
      "ns -> 南非\n",
      "ns -> 非洲\n",
      "nt -> 上市公司\n",
      "nr -> 马化腾\n",
      "nr -> 马化腾\n"
     ]
    }
   ],
   "source": [
    "# 引入词性标注接口\n",
    "import jieba.posseg as psg\n",
    " \n",
    "text = '''网友们都清楚，中国互联网行业之所以能媲美美国互联网行业，一方面是得益于中国互联网在商业模式上的创新，另一方面则是受益于资本的驱动。\n",
    "阿里巴巴这家公司早年也差钱，马云的阿里获得了孙正义软银的2000万美元投资。自此阿里巴巴的B2B业务开始进军国际市场，并在国际市场上打出了名气。\n",
    "同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展。腾讯的大股东是南非报业集团。\n",
    "大股东Naspers是一家1915年成立，总部位于南非的传媒集团。2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值。\n",
    "从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。'''\n",
    "#词性标注\n",
    "seg = psg.cut(text)\n",
    " \n",
    "#将词性标注结果打印出来\n",
    "for word,flag in seg:\n",
    "    if flag in ('ns', 'nr', 'nt'):\n",
    "        print (flag, '->', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['网友,中国互联网,行业,媲美,美国,互联网,行业,得益于,中国互联网,商业模式,创新,则是,受益,资本,驱动', '阿里巴巴,这家,公司,早年,差钱,马云,阿里,获得了,孙正义,软银,万美元,投资,自此,阿里巴巴,BB,业务,进军,国际市场,并在,国际市场,打出了,名气', '腾讯,公司,早期,获得了,资本,支持,取得了,快速发展,腾讯,大股东,南非,报业集团', '大股东,Naspers,是一家,年,成立,总部,位于,南非,传媒,集团,年,购买了,腾讯,股份,腾讯,市值,飙涨,Naspers,非洲,市值,上市公司,腾讯,股权,市值,超过了,业务,市值', '数据,可以看出,马化腾,马化腾,基金,股份,持股,不到,腾讯,早期,五虎,套现,出现在,大股东,榜单,之中']\n",
      "TfidfVector_Sklean Time:  0.0015467079999993416\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF（Term Frequency–Inverse Document Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。\n",
    "# TF-IDF是一种统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度。\n",
    "# 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "# TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。\n",
    "\n",
    "# TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n",
    "# TF-IDF实际上是：TF * IDF。\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import timeit\n",
    "vectorizer = TfidfVectorizer()   # 定义一个tf-idf的vectorizer\n",
    "\n",
    "# 构建TFIDF矩阵，方便后续对比使用\n",
    "def TfidfVector(wdlist):\n",
    "    start = timeit.default_timer()\n",
    "    X_tfidf = vectorizer.fit_transform(wdlist)   # 结果存放在X矩阵\n",
    "    stop = timeit.default_timer()\n",
    "    print('TfidfVector_Sklean Time: ', stop - start)\n",
    "    return X_tfidf\n",
    "\n",
    "def seg_depart(sentence):\n",
    "    # 对文档中的每一行进行中文分词\n",
    "\n",
    "    sentence=preprocess4wb(sentence)\n",
    "    sentence_depart = cut_wd(sentence.strip())\n",
    "    return sentence_depart\n",
    "\n",
    "#测试使用，文本首先进行上面第一步的预处理\n",
    "quelist=['网友们都清楚，中国互联网行业之所以能媲美美国互联网行业，一方面是得益于中国互联网在商业模式上的创新，另一方面则是受益于资本的驱动。',\n",
    "'阿里巴巴这家公司早年也差钱，马云的阿里获得了孙正义软银的2000万美元投资。自此阿里巴巴的B2B业务开始进军国际市场，并在国际市场上打出了名气。',\n",
    "'同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展。腾讯的大股东是南非报业集团。',\n",
    "'大股东Naspers是一家1915年成立，总部位于南非的传媒集团。2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值。',\n",
    "'从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。',]\n",
    "wdlist=[]\n",
    "for ques in quelist:\n",
    "    wd=seg_depart(ques)\n",
    "    wdstr=\",\".join(wd)\n",
    "    wdlist.append(wdstr)\n",
    "print(wdlist)\n",
    "\n",
    "X_tfidf = TfidfVector(wdlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 67)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.46310547, 0.        , 0.23155274, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.23155274,\n",
       "        0.23155274, 0.        , 0.        , 0.23155274, 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.23155274, 0.23155274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46310547, 0.        , 0.18681529,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23155274],\n",
       "       [0.20018928, 0.        , 0.20018928, 0.        , 0.        ,\n",
       "        0.16151145, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.16151145, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.        , 0.40037855, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.20018928, 0.20018928, 0.        ,\n",
       "        0.20018928, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.20018928, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20018928, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20018928, 0.16151145, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20018928, 0.20018928, 0.20018928, 0.20018928,\n",
       "        0.40037855, 0.        , 0.        , 0.        , 0.20018928,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.26179773, 0.        , 0.        ,\n",
       "        0.        , 0.26179773, 0.32449154, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21731577,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32449154, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32449154, 0.        , 0.32449154,\n",
       "        0.        , 0.        , 0.26179773, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.43463153,\n",
       "        0.        , 0.26179773, 0.        , 0.        , 0.26179773,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.32258952, 0.        , 0.16129476, 0.        ,\n",
       "        0.1301316 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16129476, 0.16129476, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.1301316 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.10802098,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.64517904,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.16129476,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.        ,\n",
       "        0.        , 0.        , 0.1301316 , 0.16129476, 0.32406294,\n",
       "        0.        , 0.        , 0.        , 0.16129476, 0.        ,\n",
       "        0.16129476, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.16129476, 0.16129476, 0.16129476, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.24846074, 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.16639706,\n",
       "        0.24846074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24846074, 0.        ,\n",
       "        0.24846074, 0.        , 0.20045656, 0.        , 0.24846074,\n",
       "        0.        , 0.        , 0.20045656, 0.        , 0.16639706,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.49692149, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00545364 -0.00149582  0.03971366  0.06125599 -0.05921654 -0.04765382\n",
      "  0.05539985  0.05652928 -0.03096416 -0.02589266  0.04575386 -0.00653471\n",
      " -0.02678722  0.03472119 -0.04110786 -0.01157157]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "# inp为输入语料, outp1为输出模型, outp2为vector格式的模型\n",
    "input_file = '../data/corpus.txt'\n",
    "out_model = '../data/corpus.model'\n",
    "out_vector = '../data/corpus.vector'\n",
    "\n",
    "# 训练skip-gram模型，16维，滑动窗口3，出现过1词就保存该词\n",
    "model = Word2Vec(LineSentence(input_file), vector_size=16, window=3, min_count=1, workers=multiprocessing.cpu_count())\n",
    "\n",
    "# 保存模型\n",
    "model.save(out_model)\n",
    "# 保存词向量\n",
    "model.wv.save_word2vec_format(out_vector, binary=False)\n",
    "\n",
    "\n",
    "#调用训练好的word2vec模型进行向量化\n",
    "\n",
    "import gensim\n",
    "word2vec = Word2Vec.load(\"../data/corpus.model\")\n",
    "\n",
    "# query=\"中国互联网行业之所以能媲美美国互联网行业\"\n",
    "# querywd=seg_depart(query)\n",
    "print(word2vec.wv['林肯'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert句向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 345/345 [00:00<00:00, 351kB/s]\n",
      "Downloading: 100%|██████████| 1.99k/1.99k [00:00<00:00, 399kB/s]\n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 405B/s]\n",
      "Downloading: 100%|██████████| 556/556 [00:00<00:00, 292kB/s]\n",
      "Downloading: 100%|██████████| 13.7M/13.7M [00:09<00:00, 1.47MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 21.8kB/s]\n",
      "Downloading: 100%|██████████| 269k/269k [00:00<00:00, 281kB/s]  \n",
      "Downloading: 100%|██████████| 19.0/19.0 [00:00<00:00, 6.98kB/s]\n",
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 152kB/s]  \n",
      "No sentence-transformers model found with name /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of emb is ===== 2\n",
      "[ 0.15300061 -0.09790983  0.86315787  0.04022079  0.34487978  0.04289062\n",
      " -0.15589474 -0.39481714  0.47034568 -0.38537207 -0.07860257  0.09301241\n",
      "  0.20630446 -0.34668624 -0.09810054  0.41245574 -0.15701818 -0.3200327\n",
      " -1.0217427  -0.05748152  0.52095217 -0.27196616  0.14191824 -0.18743883\n",
      "  0.08644073 -0.08078568 -0.01523015 -0.38654917 -0.37808013  0.0541053\n",
      "  0.40847737  0.05551349  0.06039735 -0.0669658   0.25195187 -0.16943024\n",
      "  0.17551786  0.00963114 -0.31012657  0.19530302 -0.1198843   0.09573595\n",
      " -0.05866819 -0.2962663   0.3415701  -0.3293378  -0.09927104  0.36055034\n",
      "  0.11020167  0.24922141 -0.07156698  0.25878298  0.2994059   0.07062481\n",
      " -0.2730424   0.12223209 -0.21613425 -0.26697043 -0.03332004 -0.10973013\n",
      "  0.61800194  0.26704264 -0.14203128 -0.3525691 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sentences are mapped to sentence embeddings\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 这里可以选用不同的开源出来的预训练模型，无非就是模型大小不同，最后的向量表征不同\n",
    "# embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "embedder = SentenceTransformer('hfl/chinese-electra-180g-small-generator')\n",
    "corpus=['奔驰','宝马']\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "print(\"len of emb is =====\", len(corpus_embeddings))\n",
    "print(corpus_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离线训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFA算法（文本匹配关键词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# DFA算法\n",
    "class DFAFilter():\n",
    "    def __init__(self):\n",
    "        self.keyword_chains = {}\n",
    "        self.delimit = '\\x00'\n",
    "\n",
    "    def add(self, keyword):\n",
    "        keyword = keyword.lower()\n",
    "        chars = keyword.strip()\n",
    "        if not chars:\n",
    "            return\n",
    "        level = self.keyword_chains\n",
    "        for i in range(len(chars)):\n",
    "            if chars[i] in level:\n",
    "                level = level[chars[i]]\n",
    "            else:\n",
    "                if not isinstance(level, dict):\n",
    "                    break\n",
    "                for j in range(i, len(chars)):\n",
    "                    level[chars[j]] = {}\n",
    "                    last_level, last_char = level, chars[j]\n",
    "                    level = level[chars[j]]\n",
    "                last_level[last_char] = {self.delimit: 0}\n",
    "                break\n",
    "        if i == len(chars) - 1:\n",
    "            level[self.delimit] = 0\n",
    "\n",
    "    def parse(self, path):\n",
    "        with open(path,encoding='utf-8') as f:\n",
    "            for keyword in f:\n",
    "                self.add(str(keyword).strip())\n",
    "\n",
    "    def filter(self, message,matchType):\n",
    "        message = message.lower()\n",
    "        ret = []\n",
    "        rskey=[]\n",
    "        start = 0\n",
    "        tag=0\n",
    "        while start <len(message)-1:\n",
    "            level = self.keyword_chains\n",
    "            step_ins = 0\n",
    "            for char in message[start:]:\n",
    "                if char in level:\n",
    "                    step_ins += 1\n",
    "                    if self.delimit not in level[char]:\n",
    "                        level = level[char]\n",
    "                    else:\n",
    "                        tag=1\n",
    "                        tem=step_ins\n",
    "                        #start += step_ins - 1\n",
    "                        if \"minMatch\"==matchType:\n",
    "                            rskey.append(message[start:start + step_ins])\n",
    "                            start += step_ins\n",
    "                            tag=0\n",
    "                            break\n",
    "                        level = level[char]\n",
    "                else:\n",
    "                    if tag!=0:\n",
    "                        rskey.append(message[start:start + tem])\n",
    "                        start += step_ins-1\n",
    "                    tag = 0\n",
    "                    start+=1\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(\"ok\")\n",
    "                if tag != 0:\n",
    "                    rskey.append(message[start:start + tem])\n",
    "                start += step_ins - 1\n",
    "\n",
    "                #ret.append(message[start])\n",
    "                # start += 1\n",
    "        return rskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/model/xxx.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m gfw \u001b[38;5;241m=\u001b[39m DFAFilter()\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/model/xxx.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxxx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m rs\u001b[38;5;241m=\u001b[39mgfw\u001b[38;5;241m.\u001b[39mfilter(text,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxMatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mDFAFilter.parse\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mstr\u001b[39m(keyword)\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/model/xxx.txt'"
     ]
    }
   ],
   "source": [
    "gfw = DFAFilter()\n",
    "path = \"../data/model/xxx.txt\"\n",
    "gfw.parse(path)\n",
    "text=\"xxx\"\n",
    "rs=gfw.filter(text,'maxMatch')\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取关键词算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': '马化腾', 'weight': 0.2772961200978784}, {'word': '腾讯', 'weight': 0.1626016260162572}, {'word': '五虎', 'weight': 0.1626016260162572}, {'word': '基金', 'weight': 0.14489715979786527}, {'word': '股份', 'weight': 0.14489715979786527}, {'word': '数据', 'weight': 0.0833160643714195}, {'word': '榜单', 'weight': 0.02439024390245698}]\n"
     ]
    }
   ],
   "source": [
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "# 关键词抽取\n",
    "def keywords_extraction(text):\n",
    "    tr4w = TextRank4Keyword(stop_words_file='../data/dict/Stopword.txt',allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])\n",
    "\n",
    "    #allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz']\n",
    "    # allow_speech_tags   --词性列表，用于过滤某些词性的词\n",
    "    tr4w.analyze(text=text, window=3, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.85, })\n",
    "    # text    --  文本内容，字符串\n",
    "    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2\n",
    "    # lower   --  是否将英文文本转换为小写，默认值为False\n",
    "    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点\n",
    "    #                -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'\n",
    "    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边\n",
    "    #              -- 默认值为`'no_stop_words'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。边的构造要结合`window`参数\n",
    "\n",
    "    # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85\n",
    "    keywords = tr4w.get_keywords(num=8, word_min_len=2)\n",
    "    # num           --  返回关键词数量\n",
    "    # word_min_len  --  词的最小长度，默认值为1\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# 关键短语抽取\n",
    "def keyphrases_extraction(text):\n",
    "    tr4w = TextRank4Keyword(stop_words_file='../data/dict/Stopword.txt',allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])\n",
    "    tr4w.analyze(text=text, window=3, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.35, })\n",
    "    keyphrases = tr4w.get_keyphrases(keywords_num=20, min_occur_num=1)\n",
    "    # keywords_num    --  抽取的关键词数量\n",
    "    # min_occur_num   --  关键短语在文中的最少出现次数\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "# 关键句抽取\n",
    "def keysentences_extraction(text):\n",
    "    tr4s = TextRank4Sentence()\n",
    "    tr4s.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',\n",
    "                 pagerank_config={'alpha': 0.85, })\n",
    "    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)\n",
    "    return keysentences\n",
    "\n",
    "text='从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。'\n",
    "rs=keywords_extraction(text)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('马化腾', 2.1735940914363634),\n",
       " ('五虎', 1.0377973638363636),\n",
       " ('腾讯', 0.8269687824763636),\n",
       " ('榜单', 0.8075816195600001),\n",
       " ('套现', 0.7224940288809091),\n",
       " ('持股', 0.5777534456690909),\n",
       " ('不到', 0.48511084255363635),\n",
       " ('股份', 0.4397362419445454),\n",
       " ('数据', 0.43466878894454547),\n",
       " ('基金', 0.3660047858290909)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jieba import analyse\n",
    "def getkwTfidf(text):\n",
    "    wordlist=cut_wd(text)\n",
    "    wordstext=\",\".join(wordlist)\n",
    "    tfidf = analyse.extract_tags\n",
    "    keywords = tfidf(wordstext,topK=20,withWeight=True,\n",
    "                     allowPOS=('ns', 'nr', 'nt', 'nz', 'nl', 'n', 'vn', 'vd', 'vg', 'v', 'vf', 'a', 'an', 'i'))\n",
    "    return keywords\n",
    "text='从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。'\n",
    "keywords=getkwTfidf(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of emb is ===== 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展。腾讯的大股东是南非报业集团。',\n",
       "  '大股东Naspers是一家1915年成立，总部位于南非的传媒集团。2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值。',\n",
       "  '从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。'],\n",
       " ['阿里巴巴这家公司早年也差钱，马云的阿里获得了孙正义软银的2000万美元投资。自此阿里巴巴的B2B业务开始进军国际市场，并在国际市场上打出了名气。'],\n",
       " ['网友们都清楚，中国互联网行业之所以能媲美美国互联网行业，一方面是得益于中国互联网在商业模式上的创新，另一方面则是受益于资本的驱动。']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#文本聚类句向量，然后再通过kmeans等方法进行聚类\n",
    "def getkmeans(corpus):\n",
    "    corpus_embeddings = embedder.encode(corpus)\n",
    "    print(\"len of emb is =====\", len(corpus_embeddings))\n",
    "    num_clusters = 3\n",
    "    clustering_model = KMeans(n_clusters=num_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "\n",
    "    clustered_sentences = [[] for i in range(num_clusters)]\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "    return clustered_sentences\n",
    "corpus = ['网友们都清楚，中国互联网行业之所以能媲美美国互联网行业，一方面是得益于中国互联网在商业模式上的创新，另一方面则是受益于资本的驱动。',\n",
    "'阿里巴巴这家公司早年也差钱，马云的阿里获得了孙正义软银的2000万美元投资。自此阿里巴巴的B2B业务开始进军国际市场，并在国际市场上打出了名气。',\n",
    "'同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展。腾讯的大股东是南非报业集团。',\n",
    "'大股东Naspers是一家1915年成立，总部位于南非的传媒集团。2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值。',\n",
    "'从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。',]\n",
    "cluster=getkmeans(corpus)\n",
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................\n",
      "文本已经分词完毕 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/zhangguoqiang/.cache/torch/sentence_transformers/hfl_chinese-electra-180g-small-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................\n",
      "得到的主题数量有: 1 个 ...\n",
      "............................................................................................\n",
      "\n",
      "【主题索引】:0 \n",
      "【主题语量】：5 \n",
      "【主题关键词】：腾讯,股份,早期,资本,业务,行业,市值,南非,naspers,商业模式,得益于,不到,公司,互联网,创新,上市公司,传媒,持股,孙正义,差钱 \n",
      "【主题中心句】 ：\n",
      "2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值\n",
      "从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中\n",
      "同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# singlePass算法，适合较大语料场景\n",
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "import json\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from smart_open import  smart_open\n",
    "import pandas as pd\n",
    "from  textrank4zh import TextRank4Keyword,TextRank4Sentence #关键词和关键句提取\n",
    "from tkinter import _flatten   #用于将嵌套列表压成一层\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy\n",
    "\n",
    "class Single_Pass_Cluster(object):\n",
    "    def __init__(self,\n",
    "                 filename,\n",
    "                 corpus,\n",
    "                 stop_words_file= '../data/dict/Stopword.txt',\n",
    "                 theta = 0.5):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.stop_words_file = stop_words_file\n",
    "        self.theta = theta\n",
    "        self.corpus = corpus\n",
    "\n",
    "    #'''以列表的形式读取文档'''\n",
    "    def loadData(self,filename):\n",
    "\n",
    "        Data = []\n",
    "        i = 0\n",
    "        if filename==None:\n",
    "            Data= None\n",
    "        else:\n",
    "            \n",
    "            with smart_open(self.filename,encoding='utf-8') as f:\n",
    "                tick=f.readlines()[1:10001]\n",
    "                #鉴于有些文档较长，包含多个语义中心，因此按语句结束标点进行切割获取表意单一的句子产生的聚类效果会更好\n",
    "                texts = [cut_sentences(i) for i in tick]\n",
    "                print('未切割前的语句总数有{}条...'.format(len(texts)))\n",
    "                print (\"............................................................................................\")\n",
    "                texts = [i.strip() for i in list(_flatten(texts)) if len(i)>5]\n",
    "                print('切割后的语句总数有{}条...'.format(len(texts)))\n",
    "                for line in texts:\n",
    "                    i  += 1\n",
    "                    Data.append(line )\n",
    "        return Data\n",
    "\n",
    "    def word_segment(self,texts):\n",
    "    #'''对语句进行分词，并去掉常见无意义的高频词（停用词）'''\n",
    "        stopwords = [line.strip() for line in open( self.stop_words_file,encoding='utf-8').readlines()]\n",
    "        word_segmentation = []\n",
    "        words = jieba.cut(texts)\n",
    "        for word in words:\n",
    "            if word == ' ':\n",
    "                continue\n",
    "            if word not in stopwords and word.isdigit()==False:\n",
    "                word_segmentation.append(word)\n",
    "        return word_segmentation\n",
    "\n",
    "    def get_Tfidf_vector_representation(self,word_segmentation,pivot= 10, slope = 0.1):\n",
    "        #'''采用VSM(vector space model)得到文档的空间向量表示，也可以doc2vec等算法直接获取句向量'''\n",
    "\n",
    "        dictionary = corpora.Dictionary(word_segmentation)  #获取分词后词汇和词汇id的映射关系，形成字典\n",
    "        corpus = [dictionary.doc2bow(text) for text in word_segmentation]   #得到语句的向量表示\n",
    "        tfidf = models.TfidfModel(corpus,pivot=pivot, slope =slope)      #进一步获取语句的TF-IDF向量表示\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        return corpus_tfidf\n",
    "\n",
    "    def get_bert_vector(self,word_segmentation):\n",
    "        embedder = SentenceTransformer('hfl/chinese-electra-180g-small-generator')\n",
    "        corpus_embeddings=embedder.encode(word_segmentation)\n",
    "        return corpus_embeddings\n",
    "\n",
    "    def getMaxSimilarity(self,dictTopic, vector):\n",
    "     #  '''计算新进入文档和已有文档的文本相似度，这里的相似度采用的是cosine余弦相似度，大家还可以试试\n",
    "    #kullback_leibler, jaccard, hellinger等相似度计算方法'''\n",
    "\n",
    "        maxValue = 0\n",
    "        maxIndex = -1\n",
    "        for k,cluster in dictTopic.items():\n",
    "            oneSimilarity = np.mean([matutils.cossim(vector, v) for v in cluster])\n",
    "            if oneSimilarity > maxValue:\n",
    "                maxValue = oneSimilarity\n",
    "                maxIndex = k\n",
    "        return maxIndex, maxValue\n",
    "\n",
    "    def gettopKsim(query_embedding, sentence_embeddings):\n",
    "        number_top_matches = 3\n",
    "        senindexrs = []\n",
    "        topkrs = {}\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "        results = zip(range(1, len(distances) + 1), distances)\n",
    "        results = sorted(results, key=lambda x: x[1])\n",
    "        # print(\"Query:\", query)\n",
    "        # print(\"\\nTop {} most similar sentences in corpus:\".format(number_top_matches))\n",
    "\n",
    "        for idx, distance in results[0:number_top_matches]:\n",
    "            # print(\"index\",idx, \"(Cosine Score: %.4f)\" % (1 - distance))\n",
    "            topkrs[idx] = 1 - distance\n",
    "            senindexrs.append(idx)\n",
    "        print(\"候选答案：\", topkrs)\n",
    "        return topkrs\n",
    "\n",
    "\n",
    "\n",
    "    def getMaxSimbert(self,dictTopic, vector):\n",
    "     #  '''计算新进入文档和已有文档的文本相似度，这里的相似度采用的是cosine余弦相似度，根据上一篇文章的提示，大家还可以试试\n",
    "    #kullback_leibler, jaccard, hellinger等相似度计算方法'''\n",
    "\n",
    "        maxValue = 0\n",
    "        maxIndex = -1\n",
    "        for k,cluster in dictTopic.items():\n",
    "            Similaritys =scipy.spatial.distance.cdist([vector], cluster, \"cosine\")[0]\n",
    "\n",
    "            results = sorted(Similaritys)\n",
    "            oneSimilarity=1-results[0]\n",
    "            if oneSimilarity > maxValue:\n",
    "                maxValue = oneSimilarity\n",
    "                maxIndex = k\n",
    "        return maxIndex, maxValue\n",
    "\n",
    "    def single_pass(self,corpus,texts,theta):\n",
    "        dictTopic = {}\n",
    "        clusterTopic = {}\n",
    "        numTopic = 0\n",
    "        cnt = 0\n",
    "        for vector,text in zip(corpus,texts):\n",
    "            if numTopic == 0:\n",
    "                dictTopic[numTopic] = []\n",
    "                dictTopic[numTopic].append(vector)\n",
    "                clusterTopic[numTopic] = []\n",
    "                clusterTopic[numTopic].append(text)\n",
    "                numTopic += 1\n",
    "            else:\n",
    "                #maxIndex, maxValue = self.getMaxSimilarity(dictTopic, vector)\n",
    "                maxIndex, maxValue = self.getMaxSimbert(dictTopic, vector)\n",
    "                # 以第一篇文档为种子，建立一个主题，将给定语句分配到现有的、最相似的主题中\n",
    "                if maxValue > theta:\n",
    "                    dictTopic[maxIndex].append(vector)\n",
    "                    clusterTopic[maxIndex].append(text)\n",
    "\n",
    "                # 或者创建一个新的主题\n",
    "                else:\n",
    "                    dictTopic[numTopic] = []\n",
    "                    dictTopic[numTopic].append(vector)\n",
    "                    clusterTopic[numTopic] = []\n",
    "                    clusterTopic[numTopic].append(text)\n",
    "                    numTopic += 1\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print (\"processing {}...\".format(cnt))\n",
    "        return dictTopic, clusterTopic\n",
    "\n",
    "    def fit_transform(self,theta=0.5):\n",
    "\n",
    "     # '''综合上述的函数，得出最终的聚类结果：包括聚类的标号、每个聚类的数量、关键主题词和关键语句'''\n",
    "        datMat = self.loadData(self.filename)\n",
    "        if datMat==None:\n",
    "            datMat=self.corpus\n",
    "        word_segmentation = []\n",
    "        for i in range(len(datMat)):\n",
    "            wdlist=self.word_segment(datMat[i])\n",
    "            word_segmentation.append(\"\".join(wdlist))\n",
    "\n",
    "            #word_segmentation.append(wdlist)\n",
    "        print (\"............................................................................................\")\n",
    "        print('文本已经分词完毕 !')\n",
    "\n",
    "        #得到文本数据的空间向量表示\n",
    "        #corpus_tfidf = self.get_Tfidf_vector_representation(word_segmentation)\n",
    "        corpus_bert = self.get_bert_vector(word_segmentation)\n",
    "        dictTopic, clusterTopic = self.single_pass(corpus_bert, datMat, theta)\n",
    "        print (\"............................................................................................\")\n",
    "        print( \"得到的主题数量有: {} 个 ...\".format(len(dictTopic)))\n",
    "        print (\"............................................................................................\\n\")\n",
    "        #按聚类语句数量对聚类结果进行降序排列，找到重要的聚类群\n",
    "        clusterTopic_list = sorted(clusterTopic.items(),key=lambda x: len(x[1]),reverse=True)\n",
    "        # f=open(\"../data/cluster_topic.txt\",\"w\",encoding=\"utf-8\")\n",
    "        for k in clusterTopic_list:\n",
    "            cluster_title = '\\n'.join(k[1])\n",
    "            # 得到每个聚类中的主题关键词\n",
    "            word = TextRank4Keyword()\n",
    "            word.analyze(''.join(self.word_segment(''.join(cluster_title))),window = 5,lower = True)\n",
    "            w_list = word.get_keywords(num = 20,word_min_len = 2)\n",
    "           # 得到每个聚类中的关键主题句TOP3\n",
    "            sentence = TextRank4Sentence()\n",
    "            sentence.analyze(' '.join(k[1]) ,lower = True)\n",
    "            s_list = sentence.get_key_sentences(num = 3,sentence_min_len = 3)\n",
    "            clustcenter=','.join([i.word for i in w_list])+'||'.join([i.sentence for i in s_list])\n",
    "            wrcontent=\"【主题索引】:{} \\n【主题语量】：{} \\n【主题关键词】：{} \\n【主题中心句】 ：\\n{}\".format(k[0], len(k[1]),\n",
    "                                                                              ','.join([i.word for i in w_list]),\n",
    "                                                                              '\\n'.join([i.sentence for i in s_list]))\n",
    "            # f.write(wrcontent+\"\\n\")\n",
    "            print(\"【主题索引】:{} \\n【主题语量】：{} \\n【主题关键词】：{} \\n【主题中心句】 ：\\n{}\".format(k[0], len(k[1]),\n",
    "                                                                              ','.join([i.word for i in w_list]),\n",
    "                                                                              '\\n'.join([i.sentence for i in s_list])))\n",
    "            print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = ['网友们都清楚，中国互联网行业之所以能媲美美国互联网行业，一方面是得益于中国互联网在商业模式上的创新，另一方面则是受益于资本的驱动。',\n",
    "'阿里巴巴这家公司早年也差钱，马云的阿里获得了孙正义软银的2000万美元投资。自此阿里巴巴的B2B业务开始进军国际市场，并在国际市场上打出了名气。',\n",
    "'同样的，腾讯公司在早期也是获得了资本的支持，才取得了快速发展。腾讯的大股东是南非报业集团。',\n",
    "'大股东Naspers是一家1915年成立，总部位于南非的传媒集团。2001年购买了腾讯的股份，随着腾讯市值飙涨，Naspers成为整个非洲市值最大的上市公司，其腾讯的股权市值甚至超过了自身业务的市值。',\n",
    "'从以上数据可以看出，马化腾加上马化腾基金的股份，也持股不到9%，像腾讯早期的五虎，大多也套现不少，并没有出现在10大股东的榜单之中。',]\n",
    "    single_pass_cluster = Single_Pass_Cluster(filename=None,corpus=corpus, stop_words_file='../data/dict/Stopword.txt')\n",
    "    single_pass_cluster.fit_transform(theta=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类算法（bert+finetune）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 199kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 6.14kB/s]\n",
      "Downloading: 100%|██████████| 624/624 [00:00<00:00, 159kB/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/zhangguoqiang/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "    9 训练数据\n",
      "    1 验证数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 412M/412M [00:48<00:00, 8.40MB/s] \n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/zhangguoqiang/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (21128, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                          (12, 768)\n",
      "classifier.bias                                                (12,)\n",
      "Epoch 1 / 2\n",
      "训练准确率: 0.33\n",
      "平均训练损失 loss: 2.21\n",
      "训练时间: 0:00:03\n",
      "\n",
      "测试准确率: 0.00\n",
      "平均测试损失 Loss: 2.43\n",
      "测试时间: 0:00:00\n",
      "Epoch 2 / 2\n",
      "训练准确率: 0.56\n",
      "平均训练损失 loss: 2.03\n",
      "训练时间: 0:00:03\n",
      "\n",
      "测试准确率: 0.00\n",
      "平均测试损失 Loss: 2.38\n",
      "测试时间: 0:00:00\n",
      "训练一共用了 0:00:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# 调用的bert模型名称；\n",
    "# 构造训练数据集\n",
    "# 构造input token，mask等处理\n",
    "# 超参数调整优化max_length，batch_size，epochs，lr\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import time,datetime\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # 返回 hh:mm:ss 形式的时间\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "print('Download BERT tokenizer...')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    print('There are %d GPU(s) available.' % n_gpu)\n",
    "\n",
    "    print('We will use the GPU:', [torch.cuda.get_device_name(i) for i in range(n_gpu)])\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_data = pd.read_csv(\"../data/bert_train_data/train.csv\", sep='\\t')\n",
    "desclist=train_data['desc'].unique().tolist()\n",
    "train_data['label']=train_data['desc'].apply(lambda x : desclist.index(x))\n",
    "test_data = pd.read_csv(\"../data/bert_train_data/test.csv\", sep='\\t')\n",
    "\n",
    "# 循环每一个句子...\n",
    "sentences = train_data['content'].tolist()\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=256,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # 把编码的句子加入list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # 加上 attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# 把lists 转为 tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = train_data['label'].values\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "output_dir = \"../data/bert_train_data/models/\"\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "# 代码参考 https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# 设置随机种子.\n",
    "\n",
    "# 把input 放入 TensorDataset。\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 计算 train_size 和 val_size 的长度.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# 90% 的dataset 为train_dataset, 10% 的的dataset 为val_dataset.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} 训练数据'.format(train_size))\n",
    "print('{:>5,} 验证数据'.format(val_size))\n",
    "\n",
    "# 推荐batch_size 为 16 或者 32\n",
    "batch_size = 16\n",
    "\n",
    "# 为训练数据集和验证数据集设计DataLoaders.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # 训练数据.\n",
    "            sampler = RandomSampler(train_dataset), # 打乱顺序\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # 验证数据.\n",
    "            sampler = RandomSampler(val_dataset), # 打乱顺序\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\", # 使用 12-layer 的 BERT 模型.\n",
    "    num_labels = 12, # 多分类任务的输出标签为 12个.\n",
    "    output_attentions = False, # 不返回 attentions weights.\n",
    "    output_hidden_states = False, # 不返回 all hidden-states.\n",
    ")\n",
    "model.cpu()\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "# AdamW 是一个 huggingface library 的类，'W' 是'Weight Decay fix\"的意思。\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - 默认是 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - 默认是 1e-8， 是为了防止衰减率分母除到0\n",
    "                )\n",
    "\n",
    "# bert 推荐 epochs 在2到4之间为好。\n",
    "epochs = 2\n",
    "\n",
    "# training steps 的数量: [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 设计 learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "seed_val = 2021\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "# torch.manual_seed_all(seed_val)\n",
    "\n",
    "# 记录training ,validation loss ,validation accuracy and timings.\n",
    "training_stats = []\n",
    "\n",
    "# 设置总时间.\n",
    "total_t0 = time.time()\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    # 记录每个 epoch 所用的时间\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # 每隔40个batch 输出一下所用时间.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # `batch` 包括3个 tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # 清空梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # 参考 https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        pred = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "        logits=pred.logits\n",
    "        loss=pred.loss\n",
    "        #print(loss, logits)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # backward 更新 gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # 减去大于1 的梯度，将其设为 1.0, 以防梯度爆炸.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 更新 learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        logit = logits.detach().cpu().numpy()\n",
    "        label_id = b_labels.to('cpu').numpy()\n",
    "        # 计算training 句子的准确度.\n",
    "        total_train_accuracy += flat_accuracy(logit, label_id)\n",
    "\n",
    "        # 计算batches的平均损失.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # 计算训练时间.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    # 训练集的准确率.\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    print(\"训练准确率: {0:.2f}\".format(avg_train_accuracy))\n",
    "    print(\"平均训练损失 loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"训练时间: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 设置 model 为valuation 状态，在valuation状态 dropout layers 的dropout rate会不同\n",
    "    model.eval()\n",
    "\n",
    "    # 设置参数\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # `batch` 包括3个 tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # 在valuation 状态，不更新权值，不改变计算图\n",
    "        with torch.no_grad():\n",
    "            # 参考 https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            pred = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            logits = pred.logits\n",
    "            loss = pred.loss\n",
    "\n",
    "        # 计算 validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        logit = logits.detach().cpu().numpy()\n",
    "        label_id = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # 计算 validation 句子的准确度.\n",
    "        total_eval_accuracy += flat_accuracy(logit, label_id)\n",
    "\n",
    "    # 计算 validation 的准确率.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"\")\n",
    "    print(\"测试准确率: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        torch.save(model.state_dict(), output_model_file)\n",
    "        model.config.to_json_file(output_config_file)\n",
    "        tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "    # 计算batches的平均损失.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # 计算validation 时间.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"平均测试损失 Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"测试时间: {:}\".format(validation_time))\n",
    "\n",
    "    # 记录模型参数\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"训练一共用了 {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T08:49:34.494983Z",
     "start_time": "2021-12-24T08:49:27.550022Z"
    }
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../data/bert_train_data/models'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m loadstart\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 读取模型对应的tokenizer\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/bert_train_data/models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 载入模型\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/bert_train_data/models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1734\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/transformers/utils/hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    405\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/file_download.py:1022\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m REPO_TYPES:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Accepted repo types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(REPO_TYPES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1022\u001b[0m     cache_dir, \u001b[43mrepo_folder_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(storage_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# cross platform transcription of filename, to be used as a local file path.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/utils/_validators.py:92\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m     90\u001b[0m ):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/armanaconda3/envs/py3912/lib/python3.9/site-packages/huggingface_hub-0.10.0-py3.8.egg/huggingface_hub/utils/_validators.py:136\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../data/bert_train_data/models'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "import time,datetime\n",
    "\n",
    "loadstart=time.time()\n",
    "# 读取模型对应的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('../data/bert_train_data/models')\n",
    "# 载入模型\n",
    "model = BertForSequenceClassification.from_pretrained('../data/bert_train_data/models')\n",
    "model = model.cpu()\n",
    "classifier = transformers.pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "loadend=time.time()\n",
    "print(\"load time is ====\",loadend-loadstart)\n",
    "classstart=time.time()\n",
    "texttest=''\n",
    "result=classifier(texttest)\n",
    "label2name={\"0\":\"空间\",\"1\":\"外观\"}\n",
    "labelval=result[0].get(\"label\")\n",
    "lab=labelval.split(\"_\")[1]\n",
    "rs=label2name.get(lab)\n",
    "classend=time.time()\n",
    "print(rs,\"classify time is====\",classend-classstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
